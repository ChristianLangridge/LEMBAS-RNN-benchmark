{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20001842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-computed predictions...\n",
      "âœ“ Loaded predictions for 3 models\n",
      "  Training samples: 12748, Genes: 16100\n",
      "  Test samples: 3187, Genes: 16100\n",
      "\n",
      "âœ“ All functions loaded. Ready for analysis!\n"
     ]
    }
   ],
   "source": [
    "############### LOADING DATA ###############\n",
    "\n",
    "%run '/home/christianl/Zhang-Lab/Zhang Lab Code/Boilerplate_datahandling/Remote boilerplate/model_load.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52279b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test columns type: <class 'pandas.core.indexes.base.Index'>\n",
      "First 5 column names: ['A2M', 'A2ML1', 'A3GALT2', 'A4GALT', 'AAAS']\n",
      "y_test type: <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "############### UNIT TEST ###############\n",
    "\n",
    "# Check what your y_test columns actually are:\n",
    "print(\"y_test columns type:\", type(y_test.columns))\n",
    "print(\"First 5 column names:\", y_test.columns[:5].tolist())\n",
    "\n",
    "# Also check if y_test is actually a DataFrame or numpy array:\n",
    "print(\"y_test type:\", type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7a0413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def compute_metrics_per_gene_test(y_true_df, y_pred_array):\n",
      "    \"\"\"Compute per-gene metrics.\"\"\"\n",
      "    n_genes = y_true_df.shape[1]\n",
      "    results = []\n",
      "    \n",
      "    for i, gene_name in enumerate(y_true_df.columns):\n",
      "        y_t = y_true_df.iloc[:, i].values\n",
      "        y_p = y_pred_array[:, i]\n",
      "        \n",
      "        if np.var(y_t) > 1e-10:\n",
      "            pearson_r, p_value = pearsonr(y_t, y_p)\n",
      "            r2 = r2_score(y_t, y_p)\n",
      "            \n",
      "            results.append({\n",
      "                'gene': gene_name,\n",
      "                'gene_idx': i,\n",
      "                'r2': r2,\n",
      "                'pearson_r': pearson_r,\n",
      "                'p_value': p_value\n",
      "            })\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############### UNIT TEST ###############\n",
    "\n",
    "import inspect\n",
    "print(inspect.getsource(compute_metrics_per_gene_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726bb15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test shape: (3187, 16100)\n",
      "y_pred_mlr shape: (3187, 16100)\n",
      "mlr_loaded.score: 0.7986089431408789\n",
      "compute_metrics r2: 0.9323357932034118\n",
      "compute_metrics_per_gene r2: 0        0.861767\n",
      "1        0.747973\n",
      "2        0.373796\n",
      "3        0.769920\n",
      "4        0.859807\n",
      "           ...   \n",
      "16095    0.736619\n",
      "16096    0.866326\n",
      "16097    0.924248\n",
      "16098    0.912513\n",
      "16099    0.900036\n",
      "Name: r2, Length: 16100, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "############### DIAGNOSTICS ###############\n",
    "\n",
    "# diagnostics of MLR r2 scores\n",
    "\n",
    "# checking to ensure predictions and groundtruth are comparable (they do)\n",
    "assert y_test.shape == mlr_y_pred.shape\n",
    "\n",
    "# compare r2 values\n",
    "# .score() r2 is the variance-weighted r2, which computes each gene's r2 individually (genes are treated differently) \n",
    "# as each gene predictions are still in 2D Numpy arrays not flattened\n",
    "# mlr_loaded.score: 0.7986089431408789\n",
    "print(\"mlr_loaded.score:\", mlr_loaded.score(x_test, y_test))\n",
    "\n",
    "# compute_metrics() is the flattened r2, where each gene's r2 is treated equally (flattened) before aggregating\n",
    "# compute_metrics r2: 0.9323357932034118\n",
    "\n",
    "# compute_metrics r2 > .score() r2 -> figured out that .score() when unspecified does uniform-average, different to compute_metrics r2 \n",
    "# which calculates the variance-weighted r2, taking into account the individual variances of each gene.\n",
    "# given the difference of 0.365 between the two, positive correlation between variance and R2 indicates that model performs better\n",
    "# on genes with higher variance (more distinct expression patterns) than lower variance ones (ie. housekeeping, \"silenced\", etc.)\n",
    "\n",
    "metrics_flat_mlr = compute_metrics(y_test.values, mlr_y_pred)\n",
    "print(\"compute global RÂ²:\", metrics_flat_mlr['r2'])\n",
    "\n",
    "# compute_metrics_per_gene() looks at the indiviudal r2 at per-gene resolution (using DFs to maintain biological relevance of each column) \n",
    "\n",
    "#compute_metrics_per_gene r2: 0        0.861767\n",
    "#1        0.747973\n",
    "#2        0.373796\n",
    "#3        0.769920\n",
    "#4        0.859807\n",
    "#           ...   \n",
    "#16095    0.736619\n",
    "#16096    0.866326\n",
    "#16097    0.924248\n",
    "#16098    0.912513\n",
    "#16099    0.900036\n",
    "#Name: r2, Length: 16100, dtype: float64\n",
    "\n",
    "metrics_flat_per_gene_mlr = compute_metrics_per_gene_test(y_test, mlr_y_pred)\n",
    "print(\"compute_metrics_per_gene RÂ²:\", metrics_flat_per_gene_mlr['r2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e021ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute global RÂ²: 0.9136\n"
     ]
    }
   ],
   "source": [
    "############### DIAGNOSTICS ###############\n",
    "\n",
    "# same diagnostics for XGBRF.v3 (trained on same x_train)\n",
    "# checking to ensure predictions and groundtruth are comparable\n",
    "assert y_test.shape == xgbrf_y_pred.shape\n",
    "\n",
    "# computing XGBRF metrics (aggregate and per gene (per model is this case))\n",
    "# compute global RÂ²: 0.9136\n",
    "metrics_flat_xgbrf = compute_metrics(y_test.values, xgbrf_y_pred)\n",
    "print(f\"compute global RÂ²: {metrics_flat_xgbrf['r2']:.4f}\")\n",
    "\n",
    "#compute_metrics_per_gene RÂ²: 0        0.798614\n",
    "#1        0.732539\n",
    "#2        0.390864\n",
    "#3        0.717345\n",
    "#4        0.819955\n",
    "#           ...   \n",
    "#16095    0.677608\n",
    "#16096    0.801420\n",
    "#16097    0.883455\n",
    "#16098    0.861673\n",
    "#16099    0.863954\n",
    "#Name: r2, Length: 16100, dtype: float64\n",
    "\n",
    "metrics_flat_per_gene_xgbrf = compute_metrics_per_gene_test(y_test, xgbrf_y_pred)\n",
    "print(\"compute_metrics_per_gene RÂ²:\", metrics_flat_per_gene_xgbrf['r2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c55cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute global RÂ²: 0.7366\n"
     ]
    }
   ],
   "source": [
    "############### DIAGNOSTICS ###############\n",
    "\n",
    "\n",
    "# same diagnostics for RNN.v1 (used as a reference for data preprocessing of other models )\n",
    "# checking to ensure predictions and groundtruth are comparable\n",
    "assert y_test.shape == rnn_y_pred.shape\n",
    "\n",
    "# computing RNN metrics (aggregate and per gene (per model is this case))\n",
    "# compute global RÂ²: 0.7366\n",
    "metrics_flat_rnn = compute_metrics(y_test.values, rnn_y_pred)\n",
    "print(f\"compute global RÂ²: {metrics_flat_rnn['r2']:.4f}\")\n",
    "\n",
    "#compute_metrics_per_gene RÂ²: 0        0.249111\n",
    "#1       -0.058234\n",
    "#2       -0.235948\n",
    "#3        0.382344\n",
    "#4        0.671084\n",
    "#           ...   \n",
    "#16095    0.336766\n",
    "#16096    0.509802\n",
    "#16097    0.657901\n",
    "#16098    0.514623\n",
    "#16099    0.622254\n",
    "#Name: r2, Length: 16100, dtype: float64\n",
    "\n",
    "metrics_flat_per_gene_rnn = compute_metrics_per_gene_test(y_test, rnn_y_pred)\n",
    "print(\"compute_metrics_per_gene RÂ²:\", metrics_flat_per_gene_rnn['r2'])\n",
    "\n",
    "# lower RÂ² than I would have expected - maybe the test-train splitting was different for model training? 21/01/26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "787d351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### PLOTTING FUNCTION ###############\n",
    "\n",
    "def figure_1_observed_vs_predicted(y_true, predictions_dict, \n",
    "                                  r2_method='variance_weighted',\n",
    "                                  output_path='~/Zhang-Lab/Zhang Lab Data/Saved figures/'):\n",
    "    \"\"\"\n",
    "    Generate observed vs. predicted scatterplot with Pearson correlation, R2, RMSE and MAE.\n",
    "    \"\"\"\n",
    "    set_publication_style()\n",
    "    fig, axes = plt.subplots(1, 3, figsize=FIGSIZE_TRIPLE)\n",
    "    model_names = list(predictions_dict.keys())\n",
    "    \n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        ax = axes[idx]\n",
    "        y_pred = predictions_dict[model_name]\n",
    "        \n",
    "        # Flatten arrays for scatter plot\n",
    "        y_true_flat = np.asarray(y_true).ravel()\n",
    "        y_pred_flat = np.asarray(y_pred).ravel()\n",
    "        \n",
    "        # Compute metrics based on specified method\n",
    "        if r2_method == 'variance_weighted' or r2_method == 'flattened':\n",
    "            r2 = r2_score(y_true_flat, y_pred_flat)\n",
    "            r2_label = \"RÂ² (var-w)\" # Shortened for display fit\n",
    "        elif r2_method == 'uniform_average':\n",
    "            r2 = r2_score(y_true, y_pred, multioutput='uniform_average')\n",
    "            r2_label = \"RÂ² (uni-avg)\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown r2_method: {r2_method}\")\n",
    "        \n",
    "        # Pearson correlation\n",
    "        pearson_r, p_value = pearsonr(y_true_flat, y_pred_flat)\n",
    "\n",
    "        # --- NEW: Calculate RMSE and MAE for the plot ---\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_flat, y_pred_flat))\n",
    "        mae = mean_absolute_error(y_true_flat, y_pred_flat)\n",
    "        \n",
    "        # Scatter plot\n",
    "        ax.scatter(y_true_flat, y_pred_flat, alpha=0.5, s=5, \n",
    "                   color=MODEL_COLORS.get(model_name, '#1f77b4'),\n",
    "                   edgecolors='none')\n",
    "        \n",
    "        # Perfect prediction diagonal line\n",
    "        min_val = min(y_true_flat.min(), y_pred_flat.min())\n",
    "        max_val = max(y_true_flat.max(), y_pred_flat.max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'k--', \n",
    "                lw=1, alpha=0.5, label='Perfect prediction')\n",
    "        \n",
    "        # Fit regression line\n",
    "        z = np.polyfit(y_true_flat, y_pred_flat, 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_line = np.linspace(y_true_flat.min(), y_true_flat.max(), 100)\n",
    "        y_line = p(x_line)\n",
    "        ax.plot(x_line, y_line, color=\"#000000\",\n",
    "                lw=1.5, alpha=0.8, label='Linear fit')\n",
    "        \n",
    "        # Labels and formatting\n",
    "        ax.set_xlabel('Observed Expression (Log10)', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Predicted Expression (Log10)', fontsize=12, fontweight='bold')\n",
    "        ax.set_title(model_name, fontsize=13, fontweight='bold')\n",
    "        \n",
    "        # --- MODIFIED: Add RMSE and MAE to text box ---\n",
    "        # Construct the string part by part for clarity\n",
    "        stats_text = (f\"Pearson's R = {pearson_r:.4f}\\n\"\n",
    "                      f\"{r2_label} = {r2:.4f}\\n\"\n",
    "                      f\"RMSE = {rmse:.4f}\\n\"\n",
    "                      f\"MAE = {mae:.4f}\")\n",
    "        \n",
    "        # Append p-value logic\n",
    "        if p_value < 0.001:\n",
    "            textstr = f\"{stats_text}\\np < 0.001\"\n",
    "        else:\n",
    "            textstr = f\"{stats_text}\\np = {p_value:.3f}\"\n",
    "        \n",
    "        ax.text(0.05, 0.95, textstr, transform=ax.transAxes, \n",
    "                fontsize=10, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "        \n",
    "        ax.legend(loc='lower right', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        ax.set_xlim(-8, 6)\n",
    "        ax.set_ylim(-8, 6)\n",
    "    \n",
    "    # counting how many points are within visible range\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        ax = axes[idx]\n",
    "        xlim = ax.get_xlim()\n",
    "        ylim = ax.get_ylim()\n",
    "    \n",
    "        y_pred = predictions_dict[model_name]\n",
    "        y_true_flat = np.asarray(y_true).ravel()\n",
    "        y_pred_flat = np.asarray(y_pred).ravel()\n",
    "    \n",
    "        outside_x = ((y_true_flat < xlim[0]) | (y_true_flat > xlim[1])).sum()\n",
    "        outside_y = ((y_pred_flat < ylim[0]) | (y_pred_flat > ylim[1])).sum()\n",
    "        \n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  Axis limits: x={xlim}, y={ylim}\")\n",
    "        print(f\"  Points outside x-range: {outside_x}\")\n",
    "        print(f\"  Points outside y-range: {outside_y}\")\n",
    "        print(f\"  Data range: x=[{y_true_flat.min():.2f}, {y_true_flat.max():.2f}], y=[{y_pred_flat.min():.2f}, {y_pred_flat.max():.2f}]\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=DPI, bbox_inches='tight')\n",
    "    print(f\"Figure 1 saved to {output_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Return metrics for reference\n",
    "    metrics_summary = {}\n",
    "    for model_name in model_names:\n",
    "        y_pred = predictions_dict[model_name]\n",
    "        y_true_flat = np.asarray(y_true).ravel()\n",
    "        y_pred_flat = np.asarray(y_pred).ravel()\n",
    "        \n",
    "        pearson_r, p_value = pearsonr(y_true_flat, y_pred_flat)\n",
    "        r2_variance_weighted = r2_score(y_true_flat, y_pred_flat)\n",
    "        r2_uniform = r2_score(y_true, y_pred, multioutput='uniform_average')\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_flat, y_pred_flat))\n",
    "        mae = mean_absolute_error(y_true_flat, y_pred_flat)\n",
    "        \n",
    "        metrics_summary[model_name] = {\n",
    "            'pearson_r': pearson_r,\n",
    "            'p_value': p_value,\n",
    "            'r2_variance_weighted': r2_variance_weighted,\n",
    "            'r2_uniform_average': r2_uniform,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae\n",
    "        }\n",
    "    \n",
    "    return metrics_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86867b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR predictions in visible range: 51302506 / 51310700\n",
      "Percentage visible: 99.98%\n"
     ]
    }
   ],
   "source": [
    "############### UNIT TEST ###############\n",
    "\n",
    "# How many MLR predictions are in the \"visible\" range of your plot (roughly 0-7)?\n",
    "visible_range = (mlr_y_pred >= -1) & (mlr_y_pred <= 7)\n",
    "print(f\"MLR predictions in visible range: {visible_range.sum()} / {mlr_y_pred.size}\")\n",
    "print(f\"Percentage visible: {100 * visible_range.sum() / mlr_y_pred.size:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79f33d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction variance:\n",
      "MLR: 0.6675\n",
      "XGBRFRegressor: 0.6424\n",
      "RNN: 0.5723\n"
     ]
    }
   ],
   "source": [
    "############### UNIT TEST ###############\n",
    "\n",
    "print(\"Prediction variance:\")\n",
    "for model_name, y_pred in predictions.items():\n",
    "    print(f\"{model_name}: {y_pred.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab1c7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction ranges:\n",
      "MLR: 15.0020\n",
      "XGBRFRegressor: 5.5158\n",
      "RNN: 5.3091\n"
     ]
    }
   ],
   "source": [
    "############### UNIT TEST ###############\n",
    "\n",
    "# Huge outlier range for MLR, so auto-scaling is such that the datpoints are super compressed\n",
    "# to accomodate the rare ouliers (99.98% data within a normal 0-6 range like other plots)\n",
    "\n",
    "# This is yet another reason MLR isn't a good choice for gene expression prediction tasks compared \n",
    "# to other models suitable for handling this data's non-linearity\n",
    "\n",
    "print(\"Prediction ranges:\")\n",
    "for model_name, y_pred in predictions.items():\n",
    "    pred_range = y_pred.max() - y_pred.min()\n",
    "    print(f\"{model_name}: {pred_range:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e191cbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "UNIT TEST RESULTS\n",
      "======================================================================\n",
      "\n",
      "ðŸš¨ FAILURES:\n",
      "  âŒ MLR_reasonable_range: FAIL: pred range 15.00 vs true range 5.89\n",
      "\n",
      "âš ï¸  WARNINGS:\n",
      "  âš ï¸  consistent_preprocessing: WARNING: Models may have different preprocessing. Ranges: {'MLR': (np.float64(-8.286840052861447), np.float64(6.715143860825205)), 'XGBRFRegressor': (np.float32(-1.1920929e-07), np.float32(5.5158114)), 'RNN': (np.float32(-0.70511967), np.float32(4.6039605))}\n",
      "\n",
      "âœ“ PASSED: 26/28 tests\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############### UNIT TEST ###############\n",
    "\n",
    "def test_figure_1_predictions(y_true, predictions_dict):\n",
    "    \"\"\"\n",
    "    Comprehensive sanity checks for prediction plots.\n",
    "    \n",
    "    Returns dict of test results with PASS/FAIL for each check.\n",
    "    \"\"\"\n",
    "    tests = {}\n",
    "    \n",
    "    # ===== TEST 1: Shape consistency =====\n",
    "    n_samples, n_genes = y_true.shape\n",
    "    \n",
    "    for model_name, y_pred in predictions_dict.items():\n",
    "        # Check shapes match\n",
    "        shape_match = y_pred.shape == y_true.shape\n",
    "        tests[f'{model_name}_shape_match'] = 'PASS' if shape_match else f'FAIL: {y_pred.shape} != {y_true.shape}'\n",
    "        \n",
    "        # Check for NaNs\n",
    "        has_nan = np.isnan(y_pred).any()\n",
    "        tests[f'{model_name}_no_nans'] = 'FAIL: Contains NaN' if has_nan else 'PASS'\n",
    "        \n",
    "        # Check for Infs\n",
    "        has_inf = np.isinf(y_pred).any()\n",
    "        tests[f'{model_name}_no_infs'] = 'FAIL: Contains Inf' if has_inf else 'PASS'\n",
    "    \n",
    "    # ===== TEST 2: Data range sanity =====\n",
    "    y_true_flat = y_true.ravel()\n",
    "    \n",
    "    for model_name, y_pred in predictions_dict.items():\n",
    "        y_pred_flat = y_pred.ravel()\n",
    "        \n",
    "        # Check if predictions are in reasonable range of true values\n",
    "        y_true_range = (y_true_flat.min(), y_true_flat.max())\n",
    "        y_pred_range = (y_pred_flat.min(), y_pred_flat.max())\n",
    "        \n",
    "        # Predictions shouldn't exceed true range by more than 50%\n",
    "        range_width_true = y_true_range[1] - y_true_range[0]\n",
    "        range_width_pred = y_pred_range[1] - y_pred_range[0]\n",
    "        \n",
    "        reasonable_range = range_width_pred < (range_width_true * 1.5)\n",
    "        tests[f'{model_name}_reasonable_range'] = 'PASS' if reasonable_range else \\\n",
    "            f'FAIL: pred range {range_width_pred:.2f} vs true range {range_width_true:.2f}'\n",
    "        \n",
    "        # Check if prediction distribution is similar to true distribution\n",
    "        pred_mean = y_pred_flat.mean()\n",
    "        true_mean = y_true_flat.mean()\n",
    "        mean_diff = abs(pred_mean - true_mean)\n",
    "        \n",
    "        reasonable_mean = mean_diff < (abs(true_mean) * 0.5)  # Within 50% of true mean\n",
    "        tests[f'{model_name}_reasonable_mean'] = 'PASS' if reasonable_mean else \\\n",
    "            f'FAIL: pred mean {pred_mean:.2f} vs true mean {true_mean:.2f}'\n",
    "    \n",
    "    # ===== TEST 3: Performance sanity =====\n",
    "    for model_name, y_pred in predictions_dict.items():\n",
    "        y_pred_flat = y_pred.ravel()\n",
    "        \n",
    "        # Pearson R should be positive and significant\n",
    "        pearson_r, p_value = pearsonr(y_true_flat, y_pred_flat)\n",
    "        \n",
    "        tests[f'{model_name}_positive_correlation'] = 'PASS' if pearson_r > 0 else \\\n",
    "            f'FAIL: Negative correlation {pearson_r:.4f}'\n",
    "        \n",
    "        tests[f'{model_name}_significant'] = 'PASS' if p_value < 0.05 else \\\n",
    "            f'FAIL: p-value {p_value:.4f} not significant'\n",
    "        \n",
    "        # RÂ² should be reasonable (between 0 and 1 for meaningful predictions)\n",
    "        r2 = r2_score(y_true_flat, y_pred_flat)\n",
    "        \n",
    "        tests[f'{model_name}_r2_valid'] = 'PASS' if 0 <= r2 <= 1 else \\\n",
    "            f'FAIL: RÂ² = {r2:.4f} outside [0, 1]'\n",
    "        \n",
    "        # RÂ² > 0.5 for \"good\" models (this threshold is domain-specific)\n",
    "        tests[f'{model_name}_r2_decent'] = 'PASS' if r2 > 0.5 else \\\n",
    "            f'WARNING: RÂ² = {r2:.4f} below 0.5'\n",
    "    \n",
    "    # ===== TEST 4: Cross-model consistency =====\n",
    "    model_names = list(predictions_dict.keys())\n",
    "    if len(model_names) > 1:\n",
    "        # Check if all models have similar data ranges (suggests same preprocessing)\n",
    "        ranges = {name: (pred.min(), pred.max()) for name, pred in predictions_dict.items()}\n",
    "        \n",
    "        # All ranges should be similar (within 20% of each other)\n",
    "        all_mins = [r[0] for r in ranges.values()]\n",
    "        all_maxs = [r[1] for r in ranges.values()]\n",
    "        \n",
    "        min_range = max(all_mins) - min(all_mins)\n",
    "        max_range = max(all_maxs) - min(all_maxs)\n",
    "        \n",
    "        consistent_preprocessing = (min_range < 2.0) and (max_range < 2.0)\n",
    "        \n",
    "        tests['consistent_preprocessing'] = 'PASS' if consistent_preprocessing else \\\n",
    "            f'WARNING: Models may have different preprocessing. Ranges: {ranges}'\n",
    "    \n",
    "    # ===== Print summary =====\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"UNIT TEST RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    failures = []\n",
    "    warnings = []\n",
    "    passes = []\n",
    "    \n",
    "    for test_name, result in tests.items():\n",
    "        if result.startswith('FAIL'):\n",
    "            failures.append(f\"âŒ {test_name}: {result}\")\n",
    "        elif result.startswith('WARNING'):\n",
    "            warnings.append(f\"âš ï¸  {test_name}: {result}\")\n",
    "        else:\n",
    "            passes.append(f\"âœ“ {test_name}\")\n",
    "    \n",
    "    # Print results\n",
    "    if failures:\n",
    "        print(\"\\nðŸš¨ FAILURES:\")\n",
    "        for f in failures:\n",
    "            print(f\"  {f}\")\n",
    "    \n",
    "    if warnings:\n",
    "        print(\"\\nâš ï¸  WARNINGS:\")\n",
    "        for w in warnings:\n",
    "            print(f\"  {w}\")\n",
    "    \n",
    "    if passes:\n",
    "        print(f\"\\nâœ“ PASSED: {len(passes)}/{len(tests)} tests\")\n",
    "    \n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return tests\n",
    "\n",
    "test_results = test_figure_1_predictions(y_test.values, predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aa6a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MLR:\n",
      "  Axis limits: x=(np.float64(-8.0), np.float64(6.0)), y=(np.float64(-8.0), np.float64(6.0))\n",
      "  Points outside x-range: 0\n",
      "  Points outside y-range: 12\n",
      "  Data range: x=[0.00, 5.89], y=[-8.29, 6.72]\n",
      "\n",
      "XGBRFRegressor:\n",
      "  Axis limits: x=(np.float64(-8.0), np.float64(6.0)), y=(np.float64(-8.0), np.float64(6.0))\n",
      "  Points outside x-range: 0\n",
      "  Points outside y-range: 0\n",
      "  Data range: x=[0.00, 5.89], y=[-0.00, 5.52]\n",
      "\n",
      "RNN:\n",
      "  Axis limits: x=(np.float64(-8.0), np.float64(6.0)), y=(np.float64(-8.0), np.float64(6.0))\n",
      "  Points outside x-range: 0\n",
      "  Points outside y-range: 0\n",
      "  Data range: x=[0.00, 5.89], y=[-0.71, 4.60]\n"
     ]
    }
   ],
   "source": [
    "############### PLOTTING STEP ###############\n",
    "\n",
    "figure_1_observed_vs_predicted(y_test,predictions_test,r2_method='variance_weighted', output_path = '/home/christianl/Zhang-Lab/Zhang Lab Data/Saved figures/Production_model_figures/figure1_(PRODUCTION.v4)).png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
