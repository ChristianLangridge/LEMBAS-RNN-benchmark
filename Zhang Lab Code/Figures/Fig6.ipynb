{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2ca9ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (3187, 16101)\n",
      "<class 'numpy.ndarray'> (3187, 16101)\n"
     ]
    }
   ],
   "source": [
    "%run '/home/christianl/Zhang-Lab/Zhang Lab Code/Boilerplate/Fig_config_utilities.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b556274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# wrapping models in pipelines as to avoid issues of data leakage from data centering through cross-validation analysis\n",
    "mlr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "xgbrf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', MultiOutputRegressor(\n",
    "        xgb.XGBRFRegressor(\n",
    "            random_state=42,\n",
    "            n_estimators=3,\n",
    "            n_jobs=-1,     \n",
    "            verbosity=0,\n",
    "        )\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Dictionary for cross-validation\n",
    "model_dict_cv = {\n",
    "    'MLR': mlr_pipeline,\n",
    "    'XGBRFRegressor': xgbrf_pipeline\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29c770d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# cross-validation helper function (5-fold)\n",
    "def run_cross_validation(models_dict, x, y, n_folds=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Helper function to run cross-validation for multiple models.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    models_dict : dict\n",
    "        Dictionary of models to evaluate\n",
    "        Example: {'MLR': reg_model, 'XGBRFRegressor': xgbrf_model}\n",
    "    x : np.ndarray\n",
    "        Features\n",
    "    y : np.ndarray\n",
    "        Targets\n",
    "    n_folds : int\n",
    "        Number of CV folds\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    cv_results : dict\n",
    "        Cross-validation results for each model\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"RUNNING {n_folds}-FOLD CROSS-VALIDATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    cv_results = {}\n",
    "    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    for model_name, model in models_dict.items():\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        \n",
    "        # Run cross-validation\n",
    "        scores = cross_val_score(model, x, y, cv=kfold, \n",
    "                                scoring='r2', n_jobs=-1)\n",
    "        \n",
    "        cv_results[model_name] = scores\n",
    "        \n",
    "        print(f\"  R² scores: {scores}\")\n",
    "        print(f\"  Mean: {scores.mean():.4f} (±{scores.std():.4f})\")\n",
    "    \n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5139b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RUNNING 5-FOLD CROSS-VALIDATION\n",
      "================================================================================\n",
      "\n",
      "Evaluating MLR...\n",
      "  R² scores: [0.78680039 0.77747036 0.77383015 0.77505197 0.77711993]\n",
      "  Mean: 0.7781 (±0.0046)\n",
      "\n",
      "Evaluating XGBRFRegressor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored on calling ctypes callback function: <bound method DataIter._next_wrapper of <xgboost.data.SingleBatchInternalIter object at 0x79c191d6b020>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/christianl/miniconda3/envs/remote_training/lib/python3.12/site-packages/xgboost/core.py\", line 606, in _next_wrapper\n",
      "    def _next_wrapper(self, this: None) -> int:  # pylint: disable=unused-argument\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m cv_results_models = \u001b[43mrun_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dict_cv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_train_centered\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train_centered\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mrun_cross_validation\u001b[39m\u001b[34m(models_dict, x, y, n_folds, random_state)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Run cross-validation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m scores = \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m cv_results[model_name] = scores\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  R² scores: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/remote_training/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/remote_training/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:677\u001b[39m, in \u001b[36mcross_val_score\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[32m    675\u001b[39m scorer = check_scoring(estimator, scoring=scoring)\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m cv_results = \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[33m\"\u001b[39m\u001b[33mtest_score\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/remote_training/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/remote_training/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:399\u001b[39m, in \u001b[36mcross_validate\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[32m    398\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m results = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    419\u001b[39m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[32m    421\u001b[39m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[32m    423\u001b[39m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/remote_training/lib/python3.12/site-packages/sklearn/utils/parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/remote_training/lib/python3.12/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/remote_training/lib/python3.12/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/remote_training/lib/python3.12/site-packages/joblib/parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "cv_results_models = run_cross_validation(model_dict_cv,x_train_centered,y_train_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c92448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-fold cross-validation stability plot \n",
    "def figure_6_cv_stability(cv_results_dict, \n",
    "                         output_path='/home/christianl/Zhang-Lab/Zhang Lab Code/Figures/'):\n",
    "    \"\"\"\n",
    "    Generate boxplots of model performance across CV folds.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cv_results_dict : dict\n",
    "        Dictionary where keys are model names and values are arrays of \n",
    "        cross-validation fold scores.\n",
    "        \n",
    "        Simple format (single metric):\n",
    "        {\n",
    "            'MLR': [0.75, 0.76, 0.74, 0.75, 0.76],\n",
    "            'XGBRFRegressor': [0.72, 0.73, 0.71, 0.72, 0.73]\n",
    "        }\n",
    "        \n",
    "        Or multi-metric format:\n",
    "        {\n",
    "            'MLR': {\n",
    "                'r2': [0.75, 0.76, 0.74, 0.75, 0.76],\n",
    "                'rmse': [0.42, 0.41, 0.43, 0.42, 0.41]\n",
    "            },\n",
    "            'XGBRFRegressor': {\n",
    "                'r2': [0.72, 0.73, 0.71, 0.72, 0.73],\n",
    "                'rmse': [0.45, 0.44, 0.46, 0.45, 0.44]\n",
    "            }\n",
    "        }\n",
    "    output_path : str\n",
    "        Path to save figure\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    summary_df : pd.DataFrame\n",
    "        Summary statistics for each model\n",
    "    \"\"\"\n",
    "    set_publication_style()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CROSS-VALIDATION STABILITY ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    model_names = list(cv_results_dict.keys())\n",
    "    \n",
    "    # Detect data structure\n",
    "    first_value = list(cv_results_dict.values())[0]\n",
    "    is_multi_metric = isinstance(first_value, dict)\n",
    "    \n",
    "    if is_multi_metric:\n",
    "        print(\"\\nDetected multi-metric CV results\")\n",
    "        # Extract metrics\n",
    "        available_metrics = list(first_value.keys())\n",
    "        print(f\"Available metrics: {available_metrics}\")\n",
    "        \n",
    "        # Use r2 as primary metric\n",
    "        if 'r2' in available_metrics:\n",
    "            primary_metric = 'r2'\n",
    "            metric_label = 'R²'\n",
    "        else:\n",
    "            primary_metric = available_metrics[0]\n",
    "            metric_label = primary_metric.upper()\n",
    "    else:\n",
    "        print(\"\\nDetected single-metric CV results\")\n",
    "        is_multi_metric = False\n",
    "        primary_metric = None\n",
    "        metric_label = 'R²'\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    cv_data = []\n",
    "    summary_stats = []\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        if is_multi_metric:\n",
    "            scores = cv_results_dict[model_name][primary_metric]\n",
    "        else:\n",
    "            scores = cv_results_dict[model_name]\n",
    "        \n",
    "        scores = np.array(scores)\n",
    "        \n",
    "        # Store for dataframe\n",
    "        for fold_idx, score in enumerate(scores):\n",
    "            cv_data.append({\n",
    "                'Model': model_name, \n",
    "                'Fold': fold_idx, \n",
    "                'Score': score\n",
    "            })\n",
    "        \n",
    "        # Calculate statistics\n",
    "        mean_score = np.mean(scores)\n",
    "        std_score = np.std(scores, ddof=1)  # Sample std\n",
    "        min_score = np.min(scores)\n",
    "        max_score = np.max(scores)\n",
    "        cv_coef = std_score / mean_score if mean_score != 0 else np.nan  # Coefficient of variation\n",
    "        \n",
    "        summary_stats.append({\n",
    "            'Model': model_name,\n",
    "            'Mean': mean_score,\n",
    "            'Std': std_score,\n",
    "            'Min': min_score,\n",
    "            'Max': max_score,\n",
    "            'CV': cv_coef,\n",
    "            'Range': max_score - min_score\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  Mean {metric_label}: {mean_score:.4f}\")\n",
    "        print(f\"  Std Dev:  {std_score:.4f}\")\n",
    "        print(f\"  Range:    [{min_score:.4f}, {max_score:.4f}]\")\n",
    "        print(f\"  CV (Std/Mean): {cv_coef:.4f}\")\n",
    "    \n",
    "    cv_df = pd.DataFrame(cv_data)\n",
    "    summary_df = pd.DataFrame(summary_stats)\n",
    "    \n",
    "    # LEFT PLOT: Box plot with individual points\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    # Box plot\n",
    "    bp = ax1.boxplot([cv_results_dict[m] if not is_multi_metric \n",
    "                       else cv_results_dict[m][primary_metric] \n",
    "                       for m in model_names],\n",
    "                      labels=model_names,\n",
    "                      patch_artist=True,\n",
    "                      widths=0.6,\n",
    "                      showmeans=True,\n",
    "                      meanprops=dict(marker='D', markerfacecolor='red', \n",
    "                                   markeredgecolor='red', markersize=8))\n",
    "    \n",
    "    # Color boxes\n",
    "    for patch, model_name in zip(bp['boxes'], model_names):\n",
    "        patch.set_facecolor(MODEL_COLORS.get(model_name, '#1f77b4'))\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    # Overlay individual points\n",
    "    for i, model_name in enumerate(model_names):\n",
    "        if is_multi_metric:\n",
    "            scores = cv_results_dict[model_name][primary_metric]\n",
    "        else:\n",
    "            scores = cv_results_dict[model_name]\n",
    "        \n",
    "        # Add jitter to x-coordinates\n",
    "        x = np.random.normal(i + 1, 0.04, size=len(scores))\n",
    "        ax1.scatter(x, scores, alpha=0.6, color='black', s=50, zorder=3)\n",
    "    \n",
    "    ax1.set_ylabel(f'{metric_label} Score', fontsize=12, fontweight='bold')\n",
    "    ax1.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title(f'Cross-Validation Performance ({metric_label})', \n",
    "                  fontsize=13, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # summary stats\n",
    "    summary_text = \"CV Summary:\\n\"\n",
    "    for _, row in summary_df.iterrows():\n",
    "        summary_text += f\"{row['Model']:15s}: {row['Mean']:.4f} ± {row['Std']:.4f}\\n\"\n",
    "    \n",
    "    ax1.text(0.02, 0.98, summary_text, transform=ax1.transAxes,\n",
    "            fontsize=9, verticalalignment='top', horizontalalignment='left',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.9),\n",
    "            family='monospace')\n",
    "    \n",
    "    # RIGHT PLOT: Stability comparison (Coefficient of Variation)\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    # sorted CV for better visualization\n",
    "    summary_df_sorted = summary_df.sort_values('CV')\n",
    "    \n",
    "    bars = ax2.barh(summary_df_sorted['Model'], summary_df_sorted['CV'],\n",
    "                    color=[MODEL_COLORS.get(m, '#1f77b4') for m in summary_df_sorted['Model']],\n",
    "                    alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    # value labels\n",
    "    for i, (idx, row) in enumerate(summary_df_sorted.iterrows()):\n",
    "        ax2.text(row['CV'], i, f\"  {row['CV']:.4f}\", \n",
    "                va='center', ha='left', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    #reference line for \"good\" stability (CV < 0.05 = <5% variation)\n",
    "    ax2.axvline(x=0.05, color='green', linestyle='--', linewidth=2, \n",
    "                alpha=0.7, label='Good stability (CV < 0.05)')\n",
    "    \n",
    "    ax2.set_xlabel('Coefficient of Variation (Std/Mean)', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Model', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Model Stability Across CV Folds\\n(Lower = More Stable)', \n",
    "                  fontsize=13, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='x')\n",
    "    ax2.legend(loc='lower right', fontsize=9)\n",
    "    ax2.invert_yaxis()  # best at top\n",
    "    \n",
    "    # interpretation\n",
    "    max_cv = summary_df['CV'].max()\n",
    "    if max_cv < 0.03:\n",
    "        stability_text = \"✓ All models very stable\"\n",
    "        color = 'lightgreen'\n",
    "    elif max_cv < 0.05:\n",
    "        stability_text = \"✓ All models stable\"\n",
    "        color = 'lightgreen'\n",
    "    elif max_cv < 0.10:\n",
    "        stability_text = \"~ Some instability\"\n",
    "        color = 'lightyellow'\n",
    "    else:\n",
    "        stability_text = \"⚠ High variability\"\n",
    "        color = 'lightsalmon'\n",
    "    \n",
    "    ax2.text(0.98, 0.02, stability_text, transform=ax2.transAxes,\n",
    "            fontsize=11, verticalalignment='bottom', horizontalalignment='right',\n",
    "            bbox=dict(boxstyle='round', facecolor=color, alpha=0.9),\n",
    "            fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=DPI, bbox_inches='tight')\n",
    "    print(f\"\\nFigure 6 saved to {output_path}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    plt.show()\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e1bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_6_cv_stability(cv_results_models,'figure6_v1(centered).png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
