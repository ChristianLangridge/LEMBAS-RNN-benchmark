{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20001842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (3187, 16101)\n"
     ]
    }
   ],
   "source": [
    "%run '/home/christianl/Zhang-Lab/Zhang Lab Code/Boilerplate/Fig_config_utilities.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726bb15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test shape: (3187, 16101)\n",
      "y_pred_mlr shape: (3187, 16101)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'reg_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m y_test.shape == mlr_y_pred.shape\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 2. Compare r2 from reg_test.score vs compute_metrics\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mreg_test.score:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mreg_test\u001b[49m.score(x_test, y_test))\n\u001b[32m     10\u001b[39m metrics_flat = compute_metrics(y_test_centered, mlr_y_pred)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mcompute_metrics r2:\u001b[39m\u001b[33m\"\u001b[39m, metrics_flat[\u001b[33m'\u001b[39m\u001b[33mr2\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'reg_test' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"y_pred_mlr shape:\", mlr_y_pred.shape)\n",
    "\n",
    "# 1. Verify they match\n",
    "assert y_test.shape == mlr_y_pred.shape\n",
    "\n",
    "# 2. Compare r2 from reg_test.score vs compute_metrics\n",
    "print(\"reg_test.score:\", reg_test.score(x_test, y_test))\n",
    "\n",
    "metrics_flat = compute_metrics(y_test_centered, mlr_y_pred)\n",
    "print(\"compute_metrics r2:\", metrics_flat['r2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observed vs. expected prediction scatterplots with Pearson's R for MLR and XGBRF\n",
    "\n",
    "def figure_1_observed_vs_predicted(y_true, predictions_dict, output_path='~/Zhang-Lab/Zhang Lab Data/Saved figures/figure_1.png'):\n",
    "    \"\"\"\n",
    "    Generate observed vs. predicted scatterplot with Pearson correlation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like\n",
    "        True target gene expression values\n",
    "    predictions_dict : dict\n",
    "        Dictionary with keys as model names and values as predictions\n",
    "        Example: {'RNN': pred_rnn, 'XGBRFRegressor': pred_xgb, 'Linear': pred_linear}\n",
    "    output_path : str\n",
    "        Path to save figure\n",
    "    \"\"\"\n",
    "    set_publication_style()\n",
    "    fig, axes = plt.subplots(1, 3, figsize=FIGSIZE_TRIPLE)\n",
    "    \n",
    "    model_names = list(predictions_dict.keys())\n",
    "    \n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        ax = axes[idx]\n",
    "        y_pred = predictions_dict[model_name]\n",
    "        \n",
    "        # Compute metrics\n",
    "        metrics = compute_metrics(y_true, y_pred)\n",
    "        \n",
    "        # Scatter plot\n",
    "        ax.scatter(y_true, y_pred, alpha=0.5, s=30, \n",
    "                  color=MODEL_COLORS.get(model_name, '#1f77b4'),\n",
    "                  edgecolors='none')\n",
    "        \n",
    "        # Perfect prediction diagonal line\n",
    "        min_val = min(y_true.min(), y_pred.min())\n",
    "        max_val = max(y_true.max(), y_pred.max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'k--', \n",
    "               lw=2, alpha=0.5, label='Perfect prediction')\n",
    "        \n",
    "        # Flattening arrays and fit regression line with confidence interval \n",
    "        y_true_flat = y_true.ravel()\n",
    "        y_pred_flat = y_pred.ravel()\n",
    "        z = np.polyfit(y_true_flat, y_pred_flat, 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_line = np.linspace(y_true.min(), y_true.max(), 100)\n",
    "        y_line = p(x_line)\n",
    "        ax.plot(x_line, y_line, color=MODEL_COLORS.get(model_name, '#1f77b4'),\n",
    "               lw=2.5, alpha=0.8, label='Linear fit')\n",
    "        \n",
    "        # Labels and formatting\n",
    "        ax.set_xlabel('Observed Expression', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Predicted Expression', fontsize=12, fontweight='bold')\n",
    "        ax.set_title(model_name, fontsize=13, fontweight='bold')\n",
    "        \n",
    "        # Add metrics text box\n",
    "        textstr = f\"Pearson's R = {metrics['pearson_r']:.4f}\\nAggregate r² = {metrics['r2']:.4f}\\np < 0.001\" \\\n",
    "                 if metrics['p_value'] < 0.001 else \\\n",
    "                 f\"Pearson's R = {metrics['pearson_r']:.4f}\\nAggregate r² = {metrics['r2']:.4f}\\np = {metrics['p_value']:.3f}\"\n",
    "        ax.text(0.05, 0.95, textstr, transform=ax.transAxes, \n",
    "               fontsize=10, verticalalignment='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "        \n",
    "        ax.legend(loc='lower right', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=DPI, bbox_inches='tight')\n",
    "    print(f\"Figure 1 saved to {output_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Return metrics for reference\n",
    "    metrics_summary = {}\n",
    "    for model_name in model_names:\n",
    "        metrics_summary[model_name] = compute_metrics(y_true, predictions_dict[model_name])\n",
    "    return metrics_summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
