{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20001842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (3187, 16101)\n",
      "<class 'numpy.ndarray'> (3187, 16101)\n"
     ]
    }
   ],
   "source": [
    "%run '/home/christianl/Zhang-Lab/Zhang Lab Code/Boilerplate/Fig_config_utilities.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "726bb15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test shape: (3187, 16101)\n",
      "y_pred_mlr shape: (3187, 16101)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlr_loaded.score: 0.7934193600256717\n",
      "compute_metrics r2: 0.8463157210258276\n",
      "compute_metrics_per_gene r2: 0        1.000000\n",
      "1        0.857101\n",
      "2        0.726797\n",
      "3        0.287946\n",
      "4        0.767382\n",
      "           ...   \n",
      "16096    0.776810\n",
      "16097    0.883082\n",
      "16098    0.924978\n",
      "16099    0.907058\n",
      "16100    0.893114\n",
      "Name: r2, Length: 16101, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Diagnostics of MLR r2 scores\n",
    "\n",
    "print(\"y_test shape:\", y_test_centered.shape)\n",
    "print(\"y_pred_mlr shape:\", mlr_y_pred.shape)\n",
    "\n",
    "# verify they match (they do)\n",
    "# y_test_centered shape: (3187, 16101)\n",
    "# y_pred_mlr shape: (3187, 16101)\n",
    "assert y_test_centered.shape == mlr_y_pred.shape\n",
    "\n",
    "# compare r2 values\n",
    "# .score() r2 is the variance-weighted r2, which computes each gene's r2 individually (genes are treated differently) \n",
    "# as each gene predictions are still in 2D Numpy arrays not flattened\n",
    "# reg_loaded.score: 0.7934193600256717\n",
    "print(\"mlr_loaded.score:\", mlr_loaded.score(x_test_centered, y_test_centered))\n",
    "\n",
    "# compute_metrics() is the flattened r2, where each gene's r2 is treated equally (flattened) before aggregating\n",
    "# compute_metrics r2: 0.8463157210258276\n",
    "# compute_metrics r2 > .score() r2 -> figured out that .score() when unspecified does uniform-average, different to compute_metrics r2 \n",
    "# which calculates the variance-weighted r2, taking into account the individual variances of each gene.\n",
    "# given the difference of 0.365 between the two, positive correlation between variance and R2 indicates that model performs better\n",
    "# on genes with higher variance (more distinct expression patterns) than lower variance ones (ie. housekeeping, \"silenced\", etc.)\n",
    "metrics_flat_mlr = compute_metrics(y_test_centered, mlr_y_pred)\n",
    "print(\"compute_metrics r2:\", metrics_flat_mlr['r2'])\n",
    "\n",
    "# compute_metrics_per_gene() looks at the indiviudal r2 at per-gene resolution (for extra context not hugely meaningful)\n",
    "metrics_flat_per_gene_mlr = compute_metrics_per_gene(y_test_centered, mlr_y_pred)\n",
    "print(\"compute_metrics_per_gene r2:\", metrics_flat_per_gene_mlr['r2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68e021ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred_xgbrf shape: (3187, 16101)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgbrf_loaded.score: 0.7934193600256717\n",
      "compute_metrics r2: 0.8463157210258276\n",
      "compute_metrics_per_gene r2: 0        1.000000\n",
      "1        0.857101\n",
      "2        0.726797\n",
      "3        0.287946\n",
      "4        0.767382\n",
      "           ...   \n",
      "16096    0.776810\n",
      "16097    0.883082\n",
      "16098    0.924978\n",
      "16099    0.907058\n",
      "16100    0.893114\n",
      "Name: r2, Length: 16101, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# same diagnostics for XGBRF.v3 (trained on x_train_centered)\n",
    "\n",
    "print(\"y_pred_xgbrf shape:\", xgbrf_y_pred.shape)\n",
    "print(\"xgbrf_loaded.score:\", xgbrf_loaded.score(x_test_centered, y_test_centered))\n",
    "\n",
    "metrics_flat_xgbrf = compute_metrics(y_test_centered, xgbrf_y_pred)\n",
    "print(\"compute_metrics r2:\", metrics_flat_xgbrf['r2'])\n",
    "\n",
    "metrics_flat_per_gene_xgbrf = compute_metrics_per_gene(y_test_centered, xgbrf_y_pred)\n",
    "print(\"compute_metrics_per_gene r2:\", metrics_flat_per_gene_xgbrf['r2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observed vs. expected prediction scatterplots with Pearson's R\n",
    "def figure_1_observed_vs_predicted(y_true, predictions_dict, \n",
    "                                  r2_method='variance_weighted',\n",
    "                                  output_path='~/Zhang-Lab/Zhang Lab Data/Saved figures/'):\n",
    "    \"\"\"\n",
    "    Generate observed vs. predicted scatterplot with Pearson correlation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like, shape (n_samples, n_genes)\n",
    "        True target gene expression values\n",
    "    predictions_dict : dict\n",
    "        Dictionary with keys as model names and values as predictions\n",
    "        Example: {'RNN': pred_rnn, 'XGBRFRegressor': pred_xgb, 'Linear': pred_linear}\n",
    "    r2_method : str, default='variance_weighted'\n",
    "        Method for computing R² in multi-output setting:\n",
    "        - 'variance_weighted': Weight genes by their variance (default sklearn behavior for flattened)\n",
    "        - 'uniform_average': Simple mean of per-gene R² (default sklearn .score() behavior)\n",
    "        - 'flattened': Explicitly compute on flattened data (same as variance_weighted)\n",
    "    output_path : str\n",
    "        Path to save figure\n",
    "    \"\"\"\n",
    "    set_publication_style()\n",
    "    fig, axes = plt.subplots(1, 3, figsize=FIGSIZE_TRIPLE)\n",
    "    model_names = list(predictions_dict.keys())\n",
    "    \n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        ax = axes[idx]\n",
    "        y_pred = predictions_dict[model_name]\n",
    "        \n",
    "        # Flatten arrays for scatter plot\n",
    "        y_true_flat = y_true.ravel()\n",
    "        y_pred_flat = y_pred.ravel()\n",
    "        \n",
    "        # Compute metrics based on specified method\n",
    "        if r2_method == 'variance_weighted' or r2_method == 'flattened':\n",
    "            # Compute R² on flattened data (equivalent to variance-weighted)\n",
    "            r2 = r2_score(y_true_flat, y_pred_flat)\n",
    "            r2_label = \"R² (variance-weighted)\"\n",
    "        elif r2_method == 'uniform_average':\n",
    "            # Compute mean of per-gene R²\n",
    "            r2 = r2_score(y_true, y_pred, multioutput='uniform_average')\n",
    "            r2_label = \"R² (uniform average)\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown r2_method: {r2_method}\")\n",
    "        \n",
    "        # Pearson correlation (always on flattened data for scatter plot)\n",
    "        pearson_r, p_value = pearsonr(y_true_flat, y_pred_flat)\n",
    "        \n",
    "        # Scatter plot\n",
    "        ax.scatter(y_true_flat, y_pred_flat, alpha=0.5, s=30, \n",
    "                   color=MODEL_COLORS.get(model_name, '#1f77b4'),\n",
    "                   edgecolors='none')\n",
    "        \n",
    "        # Perfect prediction diagonal line\n",
    "        min_val = min(y_true_flat.min(), y_pred_flat.min())\n",
    "        max_val = max(y_true_flat.max(), y_pred_flat.max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'k--', \n",
    "                lw=2, alpha=0.5, label='Perfect prediction')\n",
    "        \n",
    "        # Fit regression line\n",
    "        z = np.polyfit(y_true_flat, y_pred_flat, 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_line = np.linspace(y_true_flat.min(), y_true_flat.max(), 100)\n",
    "        y_line = p(x_line)\n",
    "        ax.plot(x_line, y_line, color=MODEL_COLORS.get(model_name, '#1f77b4'),\n",
    "                lw=2.5, alpha=0.8, label='Linear fit')\n",
    "        \n",
    "        # Labels and formatting\n",
    "        ax.set_xlabel('Observed Expression', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Predicted Expression', fontsize=12, fontweight='bold')\n",
    "        ax.set_title(model_name, fontsize=13, fontweight='bold')\n",
    "        \n",
    "        # Add metrics text box\n",
    "        textstr = f\"Pearson's R = {pearson_r:.4f}\\n{r2_label} = {r2:.4f}\\np < 0.001\" \\\n",
    "            if p_value < 0.001 else \\\n",
    "            f\"Pearson's R = {pearson_r:.4f}\\n{r2_label} = {r2:.4f}\\np = {p_value:.3f}\"\n",
    "        \n",
    "        ax.text(0.05, 0.95, textstr, transform=ax.transAxes, \n",
    "                fontsize=10, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "        \n",
    "        ax.legend(loc='lower right', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=DPI, bbox_inches='tight')\n",
    "    print(f\"Figure 1 saved to {output_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Return metrics for reference (including both R² methods)\n",
    "    metrics_summary = {}\n",
    "    for model_name in model_names:\n",
    "        y_pred = predictions_dict[model_name]\n",
    "        y_true_flat = y_true.ravel()\n",
    "        y_pred_flat = y_pred.ravel()\n",
    "        \n",
    "        pearson_r, p_value = pearsonr(y_true_flat, y_pred_flat)\n",
    "        r2_variance_weighted = r2_score(y_true_flat, y_pred_flat)\n",
    "        r2_uniform = r2_score(y_true, y_pred, multioutput='uniform_average')\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_flat, y_pred_flat))\n",
    "        mae = mean_absolute_error(y_true_flat, y_pred_flat)\n",
    "        \n",
    "        metrics_summary[model_name] = {\n",
    "            'pearson_r': pearson_r,\n",
    "            'p_value': p_value,\n",
    "            'r2_variance_weighted': r2_variance_weighted,\n",
    "            'r2_uniform_average': r2_uniform,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae\n",
    "        }\n",
    "    \n",
    "    return metrics_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aa6a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBRF was trained on x_test not x_test_centered so needs to be retrained for real interpretable results\n",
    "# Retrained XGBRF on centered data, redone and resaved as \"fig1_centered\"\n",
    "figure_1_observed_vs_predicted(y_test_centered,predictions,r2_method='variance_weighted', output_path = 'figure1_v3(centerd).png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
