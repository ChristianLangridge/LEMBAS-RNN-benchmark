{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e991bab8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlr_y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Zhang-Lab/Zhang Lab Code/Boilerplate_datahandling/Remote boilerplate/Fig-config_utilities(model_fitting).py:111\u001b[39m\n\u001b[32m    109\u001b[39m mlr_loaded = joblib.load(mlr_model_path)\n\u001b[32m    110\u001b[39m mlr_y_pred_train = mlr_loaded.predict(x_train)          \n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(mlr_y_pred_train), \u001b[43mmlr_y_pred\u001b[49m.shape_train)\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m##### loading XGBRF models (v4, trained on uncentered data, unified preprocessing)\u001b[39;00m\n\u001b[32m    114\u001b[39m xgbrf_model_path = \u001b[33m'\u001b[39m\u001b[33m/home/christianl/Zhang-Lab/Zhang Lab Data/Saved models/XGBRF/XGBRF_v5/all_models_batch_XGBRF[uncentered_REALFINAL].joblib\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'mlr_y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "%run '/home/christianl/Zhang-Lab/Zhang Lab Code/Boilerplate_datahandling/Remote boilerplate/Fig-config_utilities(model_fitting).py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4894fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### UNIT TEST ###############\n",
    "\n",
    "# Check what your y_test columns actually are:\n",
    "print(\"y_test columns type:\", type(y_train.columns))\n",
    "print(\"First 5 column names:\", y_train.columns[:5].tolist())\n",
    "\n",
    "# Also check if y_test is actually a DataFrame or numpy array:\n",
    "print(\"y_test type:\", type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b726b5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### DIAGNOSTICS ###############\n",
    "\n",
    "# diagnostics of MLR r2 scores for fitting to training set \n",
    "\n",
    "# checking to ensure predictions and groundtruth are comparable (they do)\n",
    "assert y_train.shape == mlr_y_pred_train.shape\n",
    "\n",
    "# compare r2 values\n",
    "# .score() r2 is the variance-weighted r2, which computes each gene's r2 individually (genes are treated differently) \n",
    "# as each gene predictions are still in 2D Numpy arrays not flattened\n",
    "# mlr_loaded.score: 0.7986089431408789\n",
    "print(\"mlr_loaded.score:\", mlr_loaded.score(x_train, y_train))\n",
    "\n",
    "# compute_metrics() is the flattened r2, where each gene's r2 is treated equally (flattened) before aggregating\n",
    "# compute_metrics r2: 0.9323357932034118\n",
    "\n",
    "# compute_metrics r2 > .score() r2 -> figured out that .score() when unspecified does uniform-average, different to compute_metrics r2 \n",
    "# which calculates the variance-weighted r2, taking into account the individual variances of each gene.\n",
    "# given the difference of 0.365 between the two, positive correlation between variance and R2 indicates that model performs better\n",
    "# on genes with higher variance (more distinct expression patterns) than lower variance ones (ie. housekeeping, \"silenced\", etc.)\n",
    "\n",
    "metrics_flat_mlr_train = compute_metrics(y_train.values, mlr_y_pred_train)\n",
    "print(\"compute global R²:\", metrics_flat_mlr_train['r2'])\n",
    "\n",
    "# compute_metrics_per_gene() looks at the indiviudal r2 at per-gene resolution (using DFs to maintain biological relevance of each column) \n",
    "\n",
    "#compute_metrics_per_gene r2: 0        0.861767\n",
    "#1        0.747973\n",
    "#2        0.373796\n",
    "#3        0.769920\n",
    "#4        0.859807\n",
    "#           ...   \n",
    "#16095    0.736619\n",
    "#16096    0.866326\n",
    "#16097    0.924248\n",
    "#16098    0.912513\n",
    "#16099    0.900036\n",
    "#Name: r2, Length: 16100, dtype: float64\n",
    "\n",
    "metrics_flat_per_gene_mlr_train = compute_metrics_per_gene_test(y_train, mlr_y_pred_train)\n",
    "print(\"compute_metrics_per_gene R²:\", metrics_flat_per_gene_mlr_train['r2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb8e332",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### DIAGNOSTICS ###############\n",
    "\n",
    "# same diagnostics for XGBRF.v3 (trained on same x_train)\n",
    "# checking to ensure predictions and groundtruth are comparable\n",
    "assert y_train.shape == xgbrf_y_pred_train.shape\n",
    "\n",
    "# computing XGBRF metrics (aggregate and per gene (per model is this case))\n",
    "# compute global R²: 0.9136\n",
    "metrics_flat_xgbrf_train = compute_metrics(y_train.values, xgbrf_y_pred_train)\n",
    "print(f\"compute global R²: {metrics_flat_xgbrf_train['r2']:.4f}\")\n",
    "\n",
    "#compute_metrics_per_gene R²: 0        0.798614\n",
    "#1        0.732539\n",
    "#2        0.390864\n",
    "#3        0.717345\n",
    "#4        0.819955\n",
    "#           ...   \n",
    "#16095    0.677608\n",
    "#16096    0.801420\n",
    "#16097    0.883455\n",
    "#16098    0.861673\n",
    "#16099    0.863954\n",
    "#Name: r2, Length: 16100, dtype: float64\n",
    "\n",
    "metrics_flat_per_gene_xgbrf_train = compute_metrics_per_gene_test(y_train, xgbrf_y_pred_train)\n",
    "print(\"compute_metrics_per_gene R²:\", metrics_flat_per_gene_xgbrf_train['r2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f342ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### DIAGNOSTICS ###############\n",
    "\n",
    "\n",
    "# same diagnostics for RNN.v1 (used as a reference for data preprocessing of other models )\n",
    "# checking to ensure predictions and groundtruth are comparable\n",
    "assert y_train.shape == rnn_y_pred_train.shape\n",
    "\n",
    "# computing RNN metrics (aggregate and per gene (per model is this case))\n",
    "# compute global R²: 0.7366\n",
    "metrics_flat_rnn_train = compute_metrics(y_train.values, rnn_y_pred_train)\n",
    "print(f\"compute global R²: {metrics_flat_rnn_train['r2']:.4f}\")\n",
    "\n",
    "#compute_metrics_per_gene R²: 0        0.249111\n",
    "#1       -0.058234\n",
    "#2       -0.235948\n",
    "#3        0.382344\n",
    "#4        0.671084\n",
    "#           ...   \n",
    "#16095    0.336766\n",
    "#16096    0.509802\n",
    "#16097    0.657901\n",
    "#16098    0.514623\n",
    "#16099    0.622254\n",
    "#Name: r2, Length: 16100, dtype: float64\n",
    "\n",
    "metrics_flat_per_gene_rnn_train = compute_metrics_per_gene_test(y_tes_train, rnn_y_pred_train)\n",
    "print(\"compute_metrics_per_gene R²:\", metrics_flat_per_gene_rnn_train['r2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a979f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
