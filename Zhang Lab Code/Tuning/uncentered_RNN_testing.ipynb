{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa30a0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing uncentered-trained RNN \n",
    "# importing pytorch\n",
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c14c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running data config file\n",
    "%run '/home/christianl/Zhang-Lab/Zhang Lab Code/Boilerplate/model_boilerplate_remote.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8655052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying testing data\n",
    "net = pd.read_csv('/home/christianl/Zhang-Lab/Zhang Lab Data/Full data files/network(full).tsv', sep = '\\t')\n",
    "x_in = x_test\n",
    "y_out = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de792894",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Quick Diagnostic: What's Actually in Your state_dict?\n",
    "======================================================\n",
    "Run this to see the exact structure of your saved model.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# Load your state_dict\n",
    "state_dict_path = '/home/christian/Zhang-Lab/Zhang Lab Data/Saved models/RNN/signaling_model.v1.pt'\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING STATE_DICT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "state_dict = torch.load(state_dict_path, map_location='cpu')\n",
    "\n",
    "print(f\"\\nType: {type(state_dict)}\")\n",
    "print(f\"\\nIs it a dict? {isinstance(state_dict, dict)}\")\n",
    "\n",
    "# Check if it's actually the full model or just state_dict\n",
    "if hasattr(state_dict, 'state_dict'):\n",
    "    print(\"\\n⚠️  This is a full model object, not just state_dict!\")\n",
    "    print(\"Extracting state_dict...\")\n",
    "    actual_state_dict = state_dict.state_dict()\n",
    "else:\n",
    "    actual_state_dict = state_dict\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEYS IN YOUR STATE_DICT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if isinstance(actual_state_dict, dict):\n",
    "    print(f\"\\nTotal keys: {len(actual_state_dict.keys())}\")\n",
    "    print(\"\\nAll keys:\")\n",
    "    for i, key in enumerate(actual_state_dict.keys(), 1):\n",
    "        print(f\"  {i}. {key}\")\n",
    "        print(f\"     Shape: {actual_state_dict[key].shape if hasattr(actual_state_dict[key], 'shape') else 'N/A'}\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  Not a dict! Type: {type(actual_state_dict)}\")\n",
    "    print(\"\\nTrying to access as object...\")\n",
    "    print(f\"Attributes: {dir(actual_state_dict)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SEARCHING FOR LAYER WEIGHTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Search for different possible key patterns\n",
    "patterns_to_check = [\n",
    "    'input_layer.weights',\n",
    "    'input_layer.weight',\n",
    "    'signaling_network.weights',\n",
    "    'signaling_network.weight',\n",
    "    'output_layer.weights',\n",
    "    'output_layer.weight',\n",
    "    'project_input',\n",
    "    'bionet',\n",
    "    'project_output',\n",
    "]\n",
    "\n",
    "print(\"\\nChecking for common patterns:\")\n",
    "for pattern in patterns_to_check:\n",
    "    found = any(pattern in key for key in actual_state_dict.keys()) if isinstance(actual_state_dict, dict) else False\n",
    "    status = \"✓ FOUND\" if found else \"✗ Not found\"\n",
    "    print(f\"  {status}: '{pattern}'\")\n",
    "    if found:\n",
    "        matching_keys = [k for k in actual_state_dict.keys() if pattern in k]\n",
    "        for key in matching_keys:\n",
    "            print(f\"      → {key}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DIAGNOSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if isinstance(actual_state_dict, dict) and len(actual_state_dict) > 0:\n",
    "    # Try to infer structure from keys\n",
    "    all_keys = list(actual_state_dict.keys())\n",
    "    \n",
    "    print(\"\\nYour state_dict structure:\")\n",
    "    \n",
    "    # Check if keys have module prefix\n",
    "    has_module_prefix = any('module.' in key for key in all_keys)\n",
    "    if has_module_prefix:\n",
    "        print(\"  ⚠️  Keys have 'module.' prefix (DataParallel model)\")\n",
    "    \n",
    "    # Check key patterns\n",
    "    if any('weight' in key for key in all_keys):\n",
    "        print(\"  ✓ Has 'weight' keys\")\n",
    "    if any('bias' in key for key in all_keys):\n",
    "        print(\"  ✓ Has 'bias' keys\")\n",
    "    if any('input' in key.lower() for key in all_keys):\n",
    "        print(\"  ✓ Has input layer keys\")\n",
    "    if any('output' in key.lower() for key in all_keys):\n",
    "        print(\"  ✓ Has output layer keys\")\n",
    "    if any('network' in key.lower() or 'bionet' in key.lower() for key in all_keys):\n",
    "        print(\"  ✓ Has network/bionet layer keys\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"NEXT STEPS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\"\"\n",
    "    Based on what we found, you may need to:\n",
    "    \n",
    "    1. If this is a full model object (not state_dict):\n",
    "       → Use: model.state_dict() to get the actual state_dict\n",
    "    \n",
    "    2. If keys have different names:\n",
    "       → I'll create an adapted loader for your specific structure\n",
    "    \n",
    "    3. If saved with DataParallel ('module.' prefix):\n",
    "       → Need to strip the prefix\n",
    "    \n",
    "    Copy the output above and I'll help you adapt the scripts!\n",
    "    \"\"\")\n",
    "\n",
    "else:\n",
    "    print(\"\"\"\n",
    "    ⚠️  Could not parse state_dict!\n",
    "    \n",
    "    This might be:\n",
    "    1. A pickled full model (not state_dict)\n",
    "    2. A different save format\n",
    "    3. Corrupted file\n",
    "    \n",
    "    Try loading with:\n",
    "    >>> model = torch.load(state_dict_path)\n",
    "    >>> print(type(model))\n",
    "    >>> print(dir(model))\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "092ef8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STATE_DICT ANALYSIS\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2640175/2818158888.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path, map_location='cpu')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'input_layer.weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43manalyze_what_you_have\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/home/christianl/Zhang-Lab/Zhang Lab Data/Saved models/RNN/signaling_model.v1.pt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36manalyze_what_you_have\u001b[39m\u001b[34m(state_dict_path)\u001b[39m\n\u001b[32m     29\u001b[39m state_dict = torch.load(state_dict_path, map_location=\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Extract dimensions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m n_ligands = \u001b[43mstate_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_layer.weights\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.shape[\u001b[32m0\u001b[39m]\n\u001b[32m     33\u001b[39m n_nodes = state_dict[\u001b[33m'\u001b[39m\u001b[33msignaling_network.weights\u001b[39m\u001b[33m'\u001b[39m].shape[\u001b[32m0\u001b[39m]\n\u001b[32m     34\u001b[39m n_tfs = state_dict[\u001b[33m'\u001b[39m\u001b[33moutput_layer.weights\u001b[39m\u001b[33m'\u001b[39m].shape[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mKeyError\u001b[39m: 'input_layer.weights'"
     ]
    }
   ],
   "source": [
    "analyze_what_you_have('/home/christianl/Zhang-Lab/Zhang Lab Data/Saved models/RNN/signaling_model.v1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization parameters\n",
    "projection_amplitude_in = 1  # default of 1 \n",
    "projection_amplitude_out = ???  # equal to the number of nodes in the final fixed network \n",
    "activation_function = 'MML'\n",
    "bionet_params = {\n",
    "    'target_steps': 100,\n",
    "    'max_steps': 300,      # RNN iterations\n",
    "    'exp_factor': 20,\n",
    "    'leak': 0.01,          # for activation function\n",
    "    'tolerance': 1e-5,     # convergence threshold\n",
    "    'spectral_target': 1e-7\n",
    "}\n",
    "ban_list = None\n",
    "\n",
    "dtype = torch.dtype=torch.float32\n",
    "device = 'cpu' # was trained on gpu \n",
    "seed = 888 # default of 888 by Kejun's self.setseeds function unless specifcied\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80ee2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in uncentered data trained model learned weighted\n",
    "uncentered_data_RNN_weights = torch.load('/home/christianl/Zhang-Lab/Zhang Lab Data/Saved models/RNN/signaling_model.v1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b270544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Model Parameters ===\")\n",
    "print(f\"Input shape: {trained_model.X_in.shape}\")\n",
    "print(f\"Output shape: {trained_model.y_out.shape}\")\n",
    "print(f\"Number of nodes: {len(trained_model.node_idx_map)}\")\n",
    "print(f\"Projection amplitude in: {trained_model.input_layer.projection_amplitude}\")\n",
    "print(f\"Projection amplitude out: {trained_model.projection_amplitude_out}\")\n",
    "print(f\"Device: {trained_model.device}\")\n",
    "print(f\"Seed: {trained_model.seed}\")\n",
    "print(f\"\\nBioNet params: {trained_model.signaling_network.training_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0ece1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting predicted values y_pred from test set \n",
    "rnn_y_pred = loaded_model.predict(x_test) \n",
    "print(type(rnn_y_pred), rnn_y_pred.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
