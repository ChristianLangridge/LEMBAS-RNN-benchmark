{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c57a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap \n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle \n",
    "from sklearn.metrics import r2_score, mean_squared_error, make_scorer\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from multiprocessing import Manager\n",
    "import optuna\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1aa3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in full data files\n",
    "gene_expression = pd.read_csv(('~/Zhang-Lab/Zhang Lab Data/Full data files/Geneexpression (full).tsv'), sep='\\t', header=0)\n",
    "tf_expression = pd.read_csv(('~/Zhang-Lab/Zhang Lab Data/Full data files/TF(full).tsv'), sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39e76b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training, testing and validation sets and into numpy arrays + combining dataframes\n",
    "x = tf_expression\n",
    "y = gene_expression\n",
    "\n",
    "combined_data = pd.concat([x, y], axis=1)\n",
    "\n",
    "# First split: 70% train and 30% temp (test + val)\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(\n",
    "    x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Second split: split the temp set into 20% test and 10% val (which is 2/3 and 1/3 of temp)\n",
    "x_test, x_val, y_test, y_val = train_test_split(\n",
    "    x_temp, y_temp, test_size=1/3, random_state=42)\n",
    "\n",
    "\n",
    "# For training set\n",
    "x_train = x_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "\n",
    "# For validation set\n",
    "x_val = x_val.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "\n",
    "# For testing set\n",
    "x_test = x_test.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d4d3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model in with pickle file \n",
    "with open('/home/christianl/Zhang-Lab/Zhang Lab Data/Saved models/Random Forest/RF_model.pkl', 'rb') as file:\n",
    "    loaded = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b99a492",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Evaluate model \n",
    "\n",
    " #loaded = pickle.load(file)\n",
    " #Loaded object is a list of models. n_models: 16101\n",
    " #y_pred.shape: (3187, 16101)\n",
    " #model.n_features_in_: None\n",
    " #model.n_outputs_: None\n",
    " #x_test.shape: (3187, 1198)\n",
    " #y_test.shape: (3187, 16101)\n",
    " #Multi-output R^2 (uniform_average): 0.7601639389405871\n",
    "\n",
    "\n",
    "# List of estimators (one per target gene) so build predictions matrix \n",
    "\n",
    "if isinstance(loaded, list):\n",
    "    models = loaded\n",
    "    print(\"Loaded object is a list of models. n_models:\", len(models))\n",
    "    y_pred = np.column_stack([m.predict(x_test) for m in models])  # (n_samples, n_genes)\n",
    "else:\n",
    "    model = loaded\n",
    "    print(\"Loaded object type:\", type(model))\n",
    "    # If single-output model but y_test is multi-column, this will raise -- handled later\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "print(\"y_pred.shape:\", y_pred.shape)\n",
    "\n",
    "# diagnostics (safe access)\n",
    "def safe_attr(obj, name):\n",
    "    return getattr(obj, name, None) if not isinstance(obj, list) else None\n",
    "\n",
    "print(\"model.n_features_in_:\", safe_attr(loaded, \"n_features_in_\"))\n",
    "print(\"model.n_outputs_:\", safe_attr(loaded, \"n_outputs_\"))\n",
    "print(\"x_test.shape:\", x_test.shape)\n",
    "print(\"y_test.shape:\", y_test.shape)\n",
    "\n",
    "# Continue with your R^2 / MSE logic expecting y_pred shape (n_samples, n_targets)\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# If y_pred is 1D, treat as single-output\n",
    "if y_pred.ndim == 1 or (y_pred.ndim == 2 and y_pred.shape[1] == 1):\n",
    "    if y_test.ndim == 2 and y_test.shape[1] > 1:\n",
    "        raise ValueError(\n",
    "            \"Model predicts a single target but y_test contains multiple targets (genes).\\n\"\n",
    "            \"Select the trained target column before splitting, e.g.:\\n\"\n",
    "            \"  target = 'GENE_NAME'\\n\"\n",
    "            \"  y = gene_expression[target]\\n\"\n",
    "            \"  then redo train_test_split and evaluation.\"\n",
    "        )\n",
    "    y_true = y_test.ravel()\n",
    "    print(\"R^2:\", r2_score(y_true, y_pred))\n",
    "    print(\"MSE:\", mean_squared_error(y_true, y_pred))\n",
    "else:\n",
    "    print(\"Multi-output R^2 (uniform_average):\", r2_score(y_test, y_pred, multioutput='uniform_average'))\n",
    "    \n",
    "    \n",
    "first = models[0]\n",
    "print(\"first estimator type:\", type(first))\n",
    "print(\"n_features_in_:\", getattr(first, \"n_features_in_\", None))\n",
    "print(\"example feature_importances_ (first 10):\", getattr(first, \"feature_importances_\", None)[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b712da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
    "\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0)\n",
    "    reg_alpha = trial.suggest_float(\"reg_alpha\", 0.0, 5.0)\n",
    "    reg_lambda = trial.suggest_float(\"reg_lambda\", 0.0, 10.0)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.1, 1.0)\n",
    "\n",
    "    # Use the RF regressor class; note the name\n",
    "    model = xgb.XGBRFRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        reg_alpha=reg_alpha,\n",
    "        reg_lambda=reg_lambda,\n",
    "        learning_rate=learning_rate,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    multi_r2 = make_scorer(r2_score, multioutput=\"uniform_average\")\n",
    "\n",
    "    score = cross_val_score(\n",
    "        model, x_train, y_train, cv=5, n_jobs=-1, scoring=multi_r2\n",
    "    ).mean()\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed07cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.RandomSampler(seed=42)) # Default is random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5484b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class TqdmCallback:\n",
    "    def __init__(self, total_trials):\n",
    "        self.pbar = tqdm(total=total_trials, desc=\"Optuna Optimization\")\n",
    "    \n",
    "    def __call__(self, study, trial):\n",
    "        self.pbar.update(1)\n",
    "        self.pbar.set_postfix({\"best_r2\": f\"{study.best_value:.4f}\"})\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.pbar.close()\n",
    "\n",
    "callback = TqdmCallback(total_trials=100)\n",
    "study.optimize(objective, n_trials=100, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ff3608",
   "metadata": {},
   "source": [
    "FULLY TEST SCRIPT FROM HERE ONWARDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "526a709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running a CPU-unburdened Optuna search\n",
    "manager = Manager()\n",
    "gpu_queue = manager.Queue()\n",
    "n_gpus = 1\n",
    "for i in range(n_gpus):\n",
    "    gpu_queue.put(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9e01164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute splits pre-optimization\n",
    "cv_indices = []\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "for train_idx, val_idx in kf.split(x_train):\n",
    "    cv_indices.append((train_idx, val_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "88a6666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the objective function\n",
    "def objective(trial):\n",
    "    gpu_id = gpu_queue.get()\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "    \n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 200),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 5),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 0.8),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 0.9),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-3, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-3, 100.0, log=True),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.03, 0.07),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        fold_scores = []\n",
    "    \n",
    "        #training each fold without recomputing splits\n",
    "        for fold, (train_idx, val_idx) in enumerate(cv_indices):\n",
    "            x_tr, x_val = x_train[train_idx], x_train[val_idx]\n",
    "            y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "            #native XGBoost is 10-15% faster than scikit wrapper\n",
    "            model = xgb.XGBRFRegressor(device=\"cuda\", tree_method=\"hist\", **params, n_jobs=1)\n",
    "            model.fit(x_tr, y_tr, verbose=False)\n",
    "        \n",
    "            y_pred = model.predict(x_val)\n",
    "            score = r2_score(y_val, y_pred, multioutput=\"uniform_average\")\n",
    "            fold_scores.append(score)\n",
    "        \n",
    "            # Pruning support\n",
    "            trial.report(score, fold)\n",
    "            if trial.should_prune():\n",
    "                gpu_queue.put(gpu_id)\n",
    "                raise optuna.TrialPruned()\n",
    "        return np.mean(fold_scores)\n",
    "    finally:\n",
    "        gpu_queue.put(gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eda599ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-06 02:16:07,443] A new study created in memory with name: no-name-c2ac8817-64f8-4dd9-82b9-f6fbc105b1c9\n"
     ]
    }
   ],
   "source": [
    "#Run optimization once at top level\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=2),\n",
    "    pruner=optuna.pruners.HyperbandPruner(\n",
    "        min_resource=1,\n",
    "        max_resource=2,  \n",
    "        reduction_factor=3,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "792056f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom callback of logs for fitting\n",
    "def log_progress(study, trial):\n",
    "    print(f\"Trial {trial.number} ended with state={trial.state}, \"\n",
    "          f\"value={trial.value}, best={study.best_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1c9e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=20,\n",
    "    n_jobs=n_gpus,            # Optuna-level parallelism\n",
    "    show_progress_bar=True,\n",
    "    callbacks=[log_progress],\n",
    "    show_progress_bar=False\n",
    ")\n",
    "\n",
    "# ---- now it's safe to access best_params ----\n",
    "if len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]) > 0:\n",
    "    best_params = study.best_params\n",
    "    print(f\"Best Hyperparameters: {best_params}\")\n",
    "else:\n",
    "    print(\"No completed trials; check logs for errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f9f8251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-12-06 02:17:01,928] Trial 0 failed with parameters: {'n_estimators': 137, 'max_depth': 5, 'subsample': 0.7195981825434216, 'colsample_bytree': 0.7394633936788146, 'reg_alpha': 0.004207988669606638, 'reg_lambda': 0.0060252157362038605, 'learning_rate': 0.03232334448672798} because of the following error: XGBoostError('[02:17:01] /home/conda/feedstock_root/build_artifacts/xgboost-split_1764148514279/work/src/common/device_vector.cu:23: Memory allocation error on worker 0: std::bad_alloc: cudaErrorMemoryAllocation: out of memory\\n- Free memory: 47MB\\n- Requested memory: 456.722MB\\n\\nStack trace:\\n  [bt] (0) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x6e) [0x7160a62d796e]\\n  [bt] (1) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(dh::detail::ThrowOOMError(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned long)+0x46d) [0x7160a6ba29cd]\\n  [bt] (2) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(thrust::THRUST_300003_SM_500_520_600_610_700_750_800_860_890_900_1000_1030_1200_1210_NS::detail::vector_base<float, dh::detail::XGBDefaultDeviceAllocatorImpl<float> >::append(unsigned long)+0x332) [0x7160a6bf1c92]\\n  [bt] (3) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(xgboost::common::SampleMean(xgboost::Context const*, bool, xgboost::linalg::Tensor<float, 2> const&, xgboost::linalg::Tensor<float, 1>*)+0xbd4) [0x7160a64a3d44]\\n  [bt] (4) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(xgboost::obj::FitInterceptGlmLike::InitEstimation(xgboost::MetaInfo const&, xgboost::linalg::Tensor<float, 1>*) const+0x61) [0x7160a67d5131]\\n  [bt] (5) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x94e) [0x7160a672c57e]\\n  [bt] (6) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(XGBoosterUpdateOneIter+0x71) [0x7160a621e9f1]\\n  [bt] (7) /home/christianl/miniconda3/envs/remote_training/lib/python3.12/lib-dynload/../../libffi.so.8(+0xa052) [0x71621bf17052]\\n  [bt] (8) /home/christianl/miniconda3/envs/remote_training/lib/python3.12/lib-dynload/../../libffi.so.8(+0x8925) [0x71621bf15925]\\n\\n').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/christianl/miniconda3/envs/remote_training/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_4023580/4189345628.py\", line 5, in timed_objective\n",
      "    score = objective(trial)\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_4023580/3736067275.py\", line 26, in objective\n",
      "    model.fit(x_tr, y_tr, verbose=False)\n",
      "  File \"/home/christianl/miniconda3/envs/remote_training/lib/python3.12/site-packages/xgboost/core.py\", line 750, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/christianl/miniconda3/envs/remote_training/lib/python3.12/site-packages/xgboost/sklearn.py\", line 2074, in fit\n",
      "    super().fit(**args)\n",
      "  File \"/home/christianl/miniconda3/envs/remote_training/lib/python3.12/site-packages/xgboost/core.py\", line 750, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/christianl/miniconda3/envs/remote_training/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1368, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"/home/christianl/miniconda3/envs/remote_training/lib/python3.12/site-packages/xgboost/core.py\", line 750, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/christianl/miniconda3/envs/remote_training/lib/python3.12/site-packages/xgboost/training.py\", line 199, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/christianl/miniconda3/envs/remote_training/lib/python3.12/site-packages/xgboost/core.py\", line 2409, in update\n",
      "    _check_call(\n",
      "  File \"/home/christianl/miniconda3/envs/remote_training/lib/python3.12/site-packages/xgboost/core.py\", line 323, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [02:17:01] /home/conda/feedstock_root/build_artifacts/xgboost-split_1764148514279/work/src/common/device_vector.cu:23: Memory allocation error on worker 0: std::bad_alloc: cudaErrorMemoryAllocation: out of memory\n",
      "- Free memory: 47MB\n",
      "- Requested memory: 456.722MB\n",
      "\n",
      "Stack trace:\n",
      "  [bt] (0) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x6e) [0x7160a62d796e]\n",
      "  [bt] (1) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(dh::detail::ThrowOOMError(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned long)+0x46d) [0x7160a6ba29cd]\n",
      "  [bt] (2) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(thrust::THRUST_300003_SM_500_520_600_610_700_750_800_860_890_900_1000_1030_1200_1210_NS::detail::vector_base<float, dh::detail::XGBDefaultDeviceAllocatorImpl<float> >::append(unsigned long)+0x332) [0x7160a6bf1c92]\n",
      "  [bt] (3) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(xgboost::common::SampleMean(xgboost::Context const*, bool, xgboost::linalg::Tensor<float, 2> const&, xgboost::linalg::Tensor<float, 1>*)+0xbd4) [0x7160a64a3d44]\n",
      "  [bt] (4) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(xgboost::obj::FitInterceptGlmLike::InitEstimation(xgboost::MetaInfo const&, xgboost::linalg::Tensor<float, 1>*) const+0x61) [0x7160a67d5131]\n",
      "  [bt] (5) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x94e) [0x7160a672c57e]\n",
      "  [bt] (6) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(XGBoosterUpdateOneIter+0x71) [0x7160a621e9f1]\n",
      "  [bt] (7) /home/christianl/miniconda3/envs/remote_training/lib/python3.12/lib-dynload/../../libffi.so.8(+0xa052) [0x71621bf17052]\n",
      "  [bt] (8) /home/christianl/miniconda3/envs/remote_training/lib/python3.12/lib-dynload/../../libffi.so.8(+0x8925) [0x71621bf15925]\n",
      "\n",
      "\n",
      "[W 2025-12-06 02:17:01,961] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[02:17:01] /home/conda/feedstock_root/build_artifacts/xgboost-split_1764148514279/work/src/common/device_vector.cu:23: Memory allocation error on worker 0: std::bad_alloc: cudaErrorMemoryAllocation: out of memory\n- Free memory: 47MB\n- Requested memory: 456.722MB\n\nStack trace:\n  [bt] (0) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x6e) [0x7160a62d796e]\n  [bt] (1) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(dh::detail::ThrowOOMError(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned long)+0x46d) [0x7160a6ba29cd]\n  [bt] (2) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(thrust::THRUST_300003_SM_500_520_600_610_700_750_800_860_890_900_1000_1030_1200_1210_NS::detail::vector_base<float, dh::detail::XGBDefaultDeviceAllocatorImpl<float> >::append(unsigned long)+0x332) [0x7160a6bf1c92]\n  [bt] (3) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(xgboost::common::SampleMean(xgboost::Context const*, bool, xgboost::linalg::Tensor<float, 2> const&, xgboost::linalg::Tensor<float, 1>*)+0xbd4) [0x7160a64a3d44]\n  [bt] (4) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(xgboost::obj::FitInterceptGlmLike::InitEstimation(xgboost::MetaInfo const&, xgboost::linalg::Tensor<float, 1>*) const+0x61) [0x7160a67d5131]\n  [bt] (5) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x94e) [0x7160a672c57e]\n  [bt] (6) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(XGBoosterUpdateOneIter+0x71) [0x7160a621e9f1]\n  [bt] (7) /home/christianl/miniconda3/envs/remote_training/lib/python3.12/lib-dynload/../../libffi.so.8(+0xa052) [0x71621bf17052]\n  [bt] (8) /home/christianl/miniconda3/envs/remote_training/lib/python3.12/lib-dynload/../../libffi.so.8(+0x8925) [0x71621bf15925]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mXGBoostError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m score\n\u001b[32m      9\u001b[39m t_start = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimed_objective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m               \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m               \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m               \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlog_progress\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m               \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mt_start\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m s\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/remote_training/lib/python3.12/site-packages/optuna/study/study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/remote_training/lib/python3.12/site-packages/optuna/study/_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/remote_training/lib/python3.12/site-packages/optuna/study/_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/remote_training/lib/python3.12/site-packages/optuna/study/_optimize.py:258\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    251\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    254\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    255\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    257\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/remote_training/lib/python3.12/site-packages/optuna/study/_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mtimed_objective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtimed_objective\u001b[39m(trial):\n\u001b[32m      4\u001b[39m     t0 = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     score = \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial.number\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mt0\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m s\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m#native XGBoost is 10-15% faster than scikit wrapper\u001b[39;00m\n\u001b[32m     25\u001b[39m model = xgb.XGBRFRegressor(device=\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m, tree_method=\u001b[33m\"\u001b[39m\u001b[33mhist\u001b[39m\u001b[33m\"\u001b[39m, **params, n_jobs=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m y_pred = model.predict(x_val)\n\u001b[32m     29\u001b[39m score = r2_score(y_val, y_pred, multioutput=\u001b[33m\"\u001b[39m\u001b[33muniform_average\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/remote_training/lib/python3.12/site-packages/xgboost/core.py:750\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    749\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/remote_training/lib/python3.12/site-packages/xgboost/sklearn.py:2074\u001b[39m, in \u001b[36mXGBRFRegressor.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   2072\u001b[39m args = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m().items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mself\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m__class__\u001b[39m\u001b[33m\"\u001b[39m)}\n\u001b[32m   2073\u001b[39m _check_rf_callback(\u001b[38;5;28mself\u001b[39m.early_stopping_rounds, \u001b[38;5;28mself\u001b[39m.callbacks)\n\u001b[32m-> \u001b[39m\u001b[32m2074\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2075\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/remote_training/lib/python3.12/site-packages/xgboost/core.py:750\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    749\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/remote_training/lib/python3.12/site-packages/xgboost/sklearn.py:1368\u001b[39m, in \u001b[36mXGBModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1366\u001b[39m     obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[38;5;28mself\u001b[39m._set_evaluation_result(evals_result)\n\u001b[32m   1383\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/remote_training/lib/python3.12/site-packages/xgboost/core.py:750\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    749\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/remote_training/lib/python3.12/site-packages/xgboost/training.py:199\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/remote_training/lib/python3.12/site-packages/xgboost/core.py:2409\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2406\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2409\u001b[39m     \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2410\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2411\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2412\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2413\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2414\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2415\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/remote_training/lib/python3.12/site-packages/xgboost/core.py:323\u001b[39m, in \u001b[36m_check_call\u001b[39m\u001b[34m(ret)\u001b[39m\n\u001b[32m    312\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[32m    313\u001b[39m \n\u001b[32m    314\u001b[39m \u001b[33;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m    return value from API calls\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret != \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "\u001b[31mXGBoostError\u001b[39m: [02:17:01] /home/conda/feedstock_root/build_artifacts/xgboost-split_1764148514279/work/src/common/device_vector.cu:23: Memory allocation error on worker 0: std::bad_alloc: cudaErrorMemoryAllocation: out of memory\n- Free memory: 47MB\n- Requested memory: 456.722MB\n\nStack trace:\n  [bt] (0) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x6e) [0x7160a62d796e]\n  [bt] (1) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(dh::detail::ThrowOOMError(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned long)+0x46d) [0x7160a6ba29cd]\n  [bt] (2) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(thrust::THRUST_300003_SM_500_520_600_610_700_750_800_860_890_900_1000_1030_1200_1210_NS::detail::vector_base<float, dh::detail::XGBDefaultDeviceAllocatorImpl<float> >::append(unsigned long)+0x332) [0x7160a6bf1c92]\n  [bt] (3) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(xgboost::common::SampleMean(xgboost::Context const*, bool, xgboost::linalg::Tensor<float, 2> const&, xgboost::linalg::Tensor<float, 1>*)+0xbd4) [0x7160a64a3d44]\n  [bt] (4) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(xgboost::obj::FitInterceptGlmLike::InitEstimation(xgboost::MetaInfo const&, xgboost::linalg::Tensor<float, 1>*) const+0x61) [0x7160a67d5131]\n  [bt] (5) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x94e) [0x7160a672c57e]\n  [bt] (6) /home/christianl/miniconda3/envs/remote_training/lib/libxgboost.so(XGBoosterUpdateOneIter+0x71) [0x7160a621e9f1]\n  [bt] (7) /home/christianl/miniconda3/envs/remote_training/lib/python3.12/lib-dynload/../../libffi.so.8(+0xa052) [0x71621bf17052]\n  [bt] (8) /home/christianl/miniconda3/envs/remote_training/lib/python3.12/lib-dynload/../../libffi.so.8(+0x8925) [0x71621bf15925]\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:39:15] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1764148514279/work/src/common/error_msg.cc:62: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def timed_objective(trial):\n",
    "    t0 = time.time()\n",
    "    score = objective(trial)\n",
    "    print(f\"Trial {trial.number} took {time.time() - t0:.1f} s\")\n",
    "    return score\n",
    "\n",
    "t_start = time.time()\n",
    "study.optimize(timed_objective, \n",
    "               n_trials=5, \n",
    "               n_jobs=n_gpus, \n",
    "               callbacks=[log_progress],\n",
    "               show_progress_bar=False)\n",
    "print(f\"Total time: {time.time() - t_start:.1f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0f7625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Training/lib/python3.13/site-packages/xgboost/training.py:199: UserWarning: [08:22:34] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1762060257953/work/src/learner.cc:790: \n",
      "Parameters: { \"max_features\", \"oob_score\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_test1 = {'n_estimators': range(20, 350, 30)}\n",
    "\n",
    "clf = xgb.XGBRFRegressor(random_state = 42,\n",
    "                         oob_score = True,\n",
    "                         max_depth = 6, \n",
    "                         max_features = 'sqrt')\n",
    "\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator=clf, \n",
    "    param_grid=param_test1,\n",
    "    scoring='r2',\n",
    "    cv=5)\n",
    "\n",
    "gsearch1.fit(x_train, y_train)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
