{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4da11ad",
   "metadata": {},
   "source": [
    "**This script contains the figure generating functions for Fig 2.A**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56f0c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json \n",
    "\n",
    "# 1. Find the Repo Root dynamically\n",
    "# Walks up folders until it finds the README.md file\n",
    "_root = next(p for p in Path.cwd().parents if (p / \"README.md\").exists())\n",
    "REPO_ROOT = str(_root)\n",
    "\n",
    "# 2. Add to sys.path so standard 'import' statements work\n",
    "import sys\n",
    "if REPO_ROOT not in sys.path:\n",
    "    sys.path.insert(0, REPO_ROOT)\n",
    "\n",
    "# Load the Data Root from the JSON file\n",
    "with open(Path(REPO_ROOT) / \"data_config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "    DATA_ROOT = config[\"DATA_ROOT\"]\n",
    "\n",
    "print(f\"Data is being pulled from: {DATA_ROOT}\")\n",
    "print(f\"Repo root identified as: {REPO_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20001842",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### GENERATE TEST/TRAIN PREDICTIONS ###############\n",
    "\n",
    "%run \"$REPO_ROOT/config/predictions/model_train_test_predictions.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b188392",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### LOADING DATA ###############\n",
    "\n",
    "%run \"$REPO_ROOT/config/predictions/model_load.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5959b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### DIAGNOSTICS ###############\n",
    "\n",
    "# 1. Create the mask (y_test is a DataFrame, so we convert to values)\n",
    "mask = y_test.values < 0\n",
    "\n",
    "# 2. Update the function to handle flattening automatically\n",
    "def get_local_metrics(y_true, y_pred, name):\n",
    "    # Ensure they are flattened and aligned with the mask\n",
    "    y_t = y_true.ravel()\n",
    "    y_p = y_pred.ravel()\n",
    "    \n",
    "    # Apply the mask to both\n",
    "    mask_flat = mask.ravel()\n",
    "    y_t_masked = y_t[mask_flat]\n",
    "    y_p_masked = y_p[mask_flat]\n",
    "    \n",
    "    mse = np.mean((y_t_masked - y_p_masked)**2)\n",
    "    pred_variance = np.var(y_p_masked) \n",
    "    \n",
    "    print(f\"--- {name} (Low Range Only) ---\")\n",
    "    print(f\"Samples in range: {len(y_t_masked)}\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"Prediction Variance: {pred_variance:.6f}\")\n",
    "    print(f\"Min Prediction: {np.min(y_p_masked):.4f}\\n\")\n",
    "\n",
    "# Use the keys from your predictions_test dictionary\n",
    "get_local_metrics(y_test.values, predictions_test[\"MLR\"], \"MLR\")\n",
    "get_local_metrics(y_test.values, predictions_test[\"XGBRFRegressor\"], \"XGBRF\")\n",
    "get_local_metrics(y_test.values, predictions_test[\"RNN\"], \"RNN\")\n",
    "\n",
    "# 3. Visualize\n",
    "plt.figure(figsize=(15, 5))\n",
    "# Mapping your dictionary keys to plot titles\n",
    "plot_info = [\n",
    "    ('MLR', predictions_test[\"MLR\"]), \n",
    "    ('XGBRF', predictions_test[\"XGBRFRegressor\"]), \n",
    "    ('RNN', predictions_test[\"RNN\"])\n",
    "]\n",
    "\n",
    "mask_flat = mask.ravel()\n",
    "y_test_flat = y_test.values.ravel()\n",
    "\n",
    "for i, (name, pred) in enumerate(plot_info, 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    pred_flat = pred.ravel()\n",
    "    \n",
    "    plt.scatter(y_test_flat[mask_flat], pred_flat[mask_flat], alpha=0.3, s=1, color=MODEL_COLORS.get(name, 'black'))\n",
    "    plt.axhline(0, color='red', linestyle='--', label='Zero Floor')\n",
    "    plt.title(f\"{name} Performance (Observed < 0)\")\n",
    "    plt.xlabel(\"Observed Expression (Log10)\")\n",
    "    plt.ylabel(\"Predicted Expression (Log10)\")\n",
    "    plt.ylim(-2, 2) # Zooming in on the boundary area\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9444291",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### DIAGNOSTICS ###############\n",
    "\n",
    "# Check the actual range of your data\n",
    "print(f\"Observed Test Min: {y_test.values.min():.6f}\")\n",
    "print(f\"Observed Test Max: {y_test.values.max():.6f}\")\n",
    "\n",
    "# Count samples in specific ranges\n",
    "print(f\"Samples exactly 0: {np.sum(y_test.values == 0)}\")\n",
    "print(f\"Samples < 0:       {np.sum(y_test.values < 0)}\")\n",
    "print(f\"Samples < 0.1:     {np.sum(y_test.values < 0.1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc98175",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### DIAGNOSTICS ###############\n",
    "\n",
    "# Calculate how much each model 'leaks' into impossible negative values\n",
    "for name, pred in predictions_test.items():\n",
    "    neg_count = np.sum(pred < 0)\n",
    "    neg_pct = (neg_count / pred.size) * 100\n",
    "    min_val = np.min(pred)\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Samples < 0: {neg_count} ({neg_pct:.2f}%)\")\n",
    "    print(f\"  Lowest Prediction: {min_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52279b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### UNIT TEST ###############\n",
    "\n",
    "# Check what your y_test columns actually are:\n",
    "print(\"y_test columns type:\", type(y_test.columns))\n",
    "print(\"First 5 column names:\", y_test.columns[:5].tolist())\n",
    "\n",
    "# Also check if y_test is actually a DataFrame or numpy array:\n",
    "print(\"y_test type:\", type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7a0413",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### UNIT TEST ###############\n",
    "\n",
    "import inspect\n",
    "print(inspect.getsource(compute_metrics_per_gene_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726bb15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### DIAGNOSTICS ###############\n",
    "\n",
    "# diagnostics of MLR r2 scores\n",
    "\n",
    "# checking to ensure predictions and groundtruth are comparable (they do)\n",
    "assert y_test.shape == mlr_y_pred.shape\n",
    "\n",
    "# compare r2 values\n",
    "# .score() r2 is the variance-weighted r2, which computes each gene's r2 individually (genes are treated differently) \n",
    "# as each gene predictions are still in 2D Numpy arrays not flattened\n",
    "# mlr_loaded.score: 0.7986089431408789\n",
    "print(\"mlr_loaded.score:\", mlr_loaded.score(x_test, y_test))\n",
    "\n",
    "# compute_metrics() is the flattened r2, where each gene's r2 is treated equally (flattened) before aggregating\n",
    "# compute_metrics r2: 0.9323357932034118\n",
    "\n",
    "# compute_metrics r2 > .score() r2 -> figured out that .score() when unspecified does uniform-average, different to compute_metrics r2 \n",
    "# which calculates the variance-weighted r2, taking into account the individual variances of each gene.\n",
    "# given the difference of 0.365 between the two, positive correlation between variance and R2 indicates that model performs better\n",
    "# on genes with higher variance (more distinct expression patterns) than lower variance ones (ie. housekeeping, \"silenced\", etc.)\n",
    "\n",
    "metrics_flat_mlr = compute_metrics(y_test.values, mlr_y_pred)\n",
    "print(\"compute global R²:\", metrics_flat_mlr['r2'])\n",
    "\n",
    "# compute_metrics_per_gene() looks at the indiviudal r2 at per-gene resolution (using DFs to maintain biological relevance of each column) \n",
    "\n",
    "#compute_metrics_per_gene r2: 0        0.861767\n",
    "#1        0.747973\n",
    "#2        0.373796\n",
    "#3        0.769920\n",
    "#4        0.859807\n",
    "#           ...   \n",
    "#16095    0.736619\n",
    "#16096    0.866326\n",
    "#16097    0.924248\n",
    "#16098    0.912513\n",
    "#16099    0.900036\n",
    "#Name: r2, Length: 16100, dtype: float64\n",
    "\n",
    "metrics_flat_per_gene_mlr = compute_metrics_per_gene_test(y_test, mlr_y_pred)\n",
    "print(\"compute_metrics_per_gene R²:\", metrics_flat_per_gene_mlr['r2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e021ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### DIAGNOSTICS ###############\n",
    "\n",
    "# same diagnostics for XGBRF.v3 (trained on same x_train)\n",
    "# checking to ensure predictions and groundtruth are comparable\n",
    "assert y_test.shape == xgbrf_y_pred.shape\n",
    "\n",
    "# computing XGBRF metrics (aggregate and per gene (per model is this case))\n",
    "# compute global R²: 0.9136\n",
    "metrics_flat_xgbrf = compute_metrics(y_test.values, xgbrf_y_pred)\n",
    "print(f\"compute global R²: {metrics_flat_xgbrf['r2']:.4f}\")\n",
    "\n",
    "#compute_metrics_per_gene R²: 0        0.798614\n",
    "#1        0.732539\n",
    "#2        0.390864\n",
    "#3        0.717345\n",
    "#4        0.819955\n",
    "#           ...   \n",
    "#16095    0.677608\n",
    "#16096    0.801420\n",
    "#16097    0.883455\n",
    "#16098    0.861673\n",
    "#16099    0.863954\n",
    "#Name: r2, Length: 16100, dtype: float64\n",
    "\n",
    "metrics_flat_per_gene_xgbrf = compute_metrics_per_gene_test(y_test, xgbrf_y_pred)\n",
    "print(\"compute_metrics_per_gene R²:\", metrics_flat_per_gene_xgbrf['r2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c55cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### DIAGNOSTICS ###############\n",
    "\n",
    "\n",
    "# same diagnostics for RNN.v1 (used as a reference for data preprocessing of other models )\n",
    "# checking to ensure predictions and groundtruth are comparable\n",
    "assert y_test.shape == rnn_y_pred.shape\n",
    "\n",
    "# computing RNN metrics (aggregate and per gene (per model is this case))\n",
    "# compute global R²: 0.7366\n",
    "metrics_flat_rnn = compute_metrics(y_test.values, rnn_y_pred)\n",
    "print(f\"compute global R²: {metrics_flat_rnn['r2']:.4f}\")\n",
    "\n",
    "#compute_metrics_per_gene R²: 0        0.249111\n",
    "#1       -0.058234\n",
    "#2       -0.235948\n",
    "#3        0.382344\n",
    "#4        0.671084\n",
    "#           ...   \n",
    "#16095    0.336766\n",
    "#16096    0.509802\n",
    "#16097    0.657901\n",
    "#16098    0.514623\n",
    "#16099    0.622254\n",
    "#Name: r2, Length: 16100, dtype: float64\n",
    "\n",
    "metrics_flat_per_gene_rnn = compute_metrics_per_gene_test(y_test, rnn_y_pred)\n",
    "print(\"compute_metrics_per_gene R²:\", metrics_flat_per_gene_rnn['r2'])\n",
    "\n",
    "# lower R² than I would have expected - maybe the test-train splitting was different for model training? 21/01/26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### PLOTTING FUNCTION ###############\n",
    "\n",
    "def figure_1_observed_vs_predicted(y_true, predictions_dict, \n",
    "                                  r2_method='variance_weighted',\n",
    "                                  output_path=f\"{DATA_ROOT}/Saved figures/\"):\n",
    "    \"\"\"\n",
    "    Generate observed vs. predicted scatterplot with Pearson correlation, R2, RMSE and MAE.\n",
    "    \"\"\"\n",
    "    set_publication_style()\n",
    "    fig, axes = plt.subplots(1, 3, figsize=FIGSIZE_TRIPLE)\n",
    "    model_names = list(predictions_dict.keys())\n",
    "    \n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        ax = axes[idx]\n",
    "        y_pred = predictions_dict[model_name]\n",
    "        \n",
    "        # Flatten arrays for scatter plot\n",
    "        y_true_flat = np.asarray(y_true).ravel()\n",
    "        y_pred_flat = np.asarray(y_pred).ravel()\n",
    "        \n",
    "        # Compute metrics based on specified method\n",
    "        if r2_method == 'variance_weighted' or r2_method == 'flattened':\n",
    "            r2 = r2_score(y_true_flat, y_pred_flat)\n",
    "            r2_label = \"R² (var-w)\" # Shortened for display fit\n",
    "        elif r2_method == 'uniform_average':\n",
    "            r2 = r2_score(y_true, y_pred, multioutput='uniform_average')\n",
    "            r2_label = \"R² (uni-avg)\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown r2_method: {r2_method}\")\n",
    "        \n",
    "        # Pearson correlation\n",
    "        pearson_r, p_value = pearsonr(y_true_flat, y_pred_flat)\n",
    "\n",
    "        # --- NEW: Calculate RMSE and MAE for the plot ---\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_flat, y_pred_flat))\n",
    "        mae = mean_absolute_error(y_true_flat, y_pred_flat)\n",
    "        \n",
    "        # Scatter plot\n",
    "        ax.scatter(y_true_flat, y_pred_flat, alpha=0.5, s=5, \n",
    "                   color=MODEL_COLORS.get(model_name, '#1f77b4'),\n",
    "                   edgecolors='none')\n",
    "        \n",
    "        # Perfect prediction diagonal line\n",
    "        min_val = min(y_true_flat.min(), y_pred_flat.min())\n",
    "        max_val = max(y_true_flat.max(), y_pred_flat.max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'k--', \n",
    "                lw=1, alpha=0.5, label='Perfect prediction')\n",
    "        \n",
    "        # Fit regression line\n",
    "        z = np.polyfit(y_true_flat, y_pred_flat, 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_line = np.linspace(y_true_flat.min(), y_true_flat.max(), 100)\n",
    "        y_line = p(x_line)\n",
    "        ax.plot(x_line, y_line, color=\"#000000\",\n",
    "                lw=1.5, alpha=0.8, label='Linear fit')\n",
    "        \n",
    "        # Labels and formatting\n",
    "        ax.set_xlabel('Observed Expression (Log10)', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Predicted Expression (Log10)', fontsize=12, fontweight='bold')\n",
    "        ax.set_title(model_name, fontsize=13, fontweight='bold')\n",
    "        \n",
    "        # --- MODIFIED: Add RMSE and MAE to text box ---\n",
    "        # Construct the string part by part for clarity\n",
    "        stats_text = (f\"Pearson's R = {pearson_r:.4f}\\n\"\n",
    "                      f\"{r2_label} = {r2:.4f}\\n\"\n",
    "                      f\"RMSE = {rmse:.4f}\\n\"\n",
    "                      f\"MAE = {mae:.4f}\")\n",
    "        \n",
    "        # Append p-value logic\n",
    "        if p_value < 0.001:\n",
    "            textstr = f\"{stats_text}\\np < 0.001\"\n",
    "        else:\n",
    "            textstr = f\"{stats_text}\\np = {p_value:.3f}\"\n",
    "        \n",
    "        ax.text(0.05, 0.95, textstr, transform=ax.transAxes, \n",
    "                fontsize=10, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "        \n",
    "        ax.legend(loc='lower right', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        ax.set_xlim(-8, 6)\n",
    "        ax.set_ylim(-8, 6)\n",
    "    \n",
    "    # counting how many points are within visible range\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        ax = axes[idx]\n",
    "        xlim = ax.get_xlim()\n",
    "        ylim = ax.get_ylim()\n",
    "    \n",
    "        y_pred = predictions_dict[model_name]\n",
    "        y_true_flat = np.asarray(y_true).ravel()\n",
    "        y_pred_flat = np.asarray(y_pred).ravel()\n",
    "    \n",
    "        outside_x = ((y_true_flat < xlim[0]) | (y_true_flat > xlim[1])).sum()\n",
    "        outside_y = ((y_pred_flat < ylim[0]) | (y_pred_flat > ylim[1])).sum()\n",
    "        \n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  Axis limits: x={xlim}, y={ylim}\")\n",
    "        print(f\"  Points outside x-range: {outside_x}\")\n",
    "        print(f\"  Points outside y-range: {outside_y}\")\n",
    "        print(f\"  Data range: x=[{y_true_flat.min():.2f}, {y_true_flat.max():.2f}], y=[{y_pred_flat.min():.2f}, {y_pred_flat.max():.2f}]\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=DPI, bbox_inches='tight')\n",
    "    print(f\"Figure 1 saved to {output_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Return metrics for reference\n",
    "    metrics_summary = {}\n",
    "    for model_name in model_names:\n",
    "        y_pred = predictions_dict[model_name]\n",
    "        y_true_flat = np.asarray(y_true).ravel()\n",
    "        y_pred_flat = np.asarray(y_pred).ravel()\n",
    "        \n",
    "        pearson_r, p_value = pearsonr(y_true_flat, y_pred_flat)\n",
    "        r2_variance_weighted = r2_score(y_true_flat, y_pred_flat)\n",
    "        r2_uniform = r2_score(y_true, y_pred, multioutput='uniform_average')\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_flat, y_pred_flat))\n",
    "        mae = mean_absolute_error(y_true_flat, y_pred_flat)\n",
    "        \n",
    "        metrics_summary[model_name] = {\n",
    "            'pearson_r': pearson_r,\n",
    "            'p_value': p_value,\n",
    "            'r2_variance_weighted': r2_variance_weighted,\n",
    "            'r2_uniform_average': r2_uniform,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae\n",
    "        }\n",
    "    \n",
    "    return metrics_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86867b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### UNIT TEST ###############\n",
    "\n",
    "# How many MLR predictions are in the \"visible\" range of your plot (roughly 0-7)?\n",
    "visible_range = (mlr_y_pred_test >= -1) & (mlr_y_pred_test <= 7)\n",
    "print(f\"MLR predictions in visible range: {visible_range.sum()} / {mlr_y_pred_test.size}\")\n",
    "print(f\"Percentage visible: {100 * visible_range.sum() / mlr_y_pred_test.size:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f33d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### UNIT TEST ###############\n",
    "\n",
    "print(\"Prediction variance:\")\n",
    "for model_name, y_pred in predictions_test.items():\n",
    "    print(f\"{model_name}: {y_pred.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab1c7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### UNIT TEST ###############\n",
    "\n",
    "# Huge outlier range for MLR, so auto-scaling is such that the datpoints are super compressed\n",
    "# to accomodate the rare ouliers (99.98% data within a normal 0-6 range like other plots)\n",
    "\n",
    "# This is yet another reason MLR isn't a good choice for gene expression prediction tasks compared \n",
    "# to other models suitable for handling this data's non-linearity\n",
    "\n",
    "print(\"Prediction ranges:\")\n",
    "for model_name, y_pred in predictions.items():\n",
    "    pred_range = y_pred.max() - y_pred.min()\n",
    "    print(f\"{model_name}: {pred_range:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aa6a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### PLOTTING STEP ###############\n",
    "\n",
    "figure_1_observed_vs_predicted(y_test, predictions_test, r2_method='variance_weighted', output_path=f\"{DATA_ROOT}/Saved figures/Production_model_figures/figure1_(PRODUCTION.v4).png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}