{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "547452c9",
   "metadata": {},
   "source": [
    "**This script contains the figure generating functions for Fig 3.A**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "133b4d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is being pulled from: /home/christianl/Zhang-Lab/Zhang Lab Data\n",
      "Repo root identified as: /home/christianl/Zhang-Lab\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json \n",
    "\n",
    "# 1. Find the Repo Root dynamically\n",
    "# Walks up folders until it finds the README.md file\n",
    "_root = next(p for p in Path.cwd().parents if (p / \"README.md\").exists())\n",
    "REPO_ROOT = str(_root)\n",
    "\n",
    "# 2. Add to sys.path so standard 'import' statements work\n",
    "import sys\n",
    "if REPO_ROOT not in sys.path:\n",
    "    sys.path.insert(0, REPO_ROOT)\n",
    "\n",
    "# Load the Data Root from the JSON file\n",
    "with open(Path(REPO_ROOT) / \"data_config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "    DATA_ROOT = config[\"DATA_ROOT\"]\n",
    "\n",
    "print(f\"Data is being pulled from: {DATA_ROOT}\")\n",
    "print(f\"Repo root identified as: {REPO_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecccece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### GENERATE VALIDATION PREDICTIONS ###############\n",
    "\n",
    "%run \"$REPO_ROOT/config/predictions/model_validation_predictions.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aa5d95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-computed predictions...\n",
      "✓ Loaded predictions for 3 models\n",
      "  Training samples: 12748, Genes: 16100\n",
      "  Test samples: 3187, Genes: 16100\n",
      "  Val samples: 262, Genes: 16100\n",
      "\n",
      "✓ All functions loaded. Ready for analysis!\n"
     ]
    }
   ],
   "source": [
    "############### LOADING DATA ###############\n",
    "\n",
    "%run \"$REPO_ROOT/config/predictions/model_load.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9056e08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### UNIT TEST ###############\n",
    "\n",
    "# Check what your y_validation columns actually are:\n",
    "print(\"y_validation columns type:\", type(y_validation.columns))\n",
    "print(\"First 5 column names:\", y_validation.columns[:5].tolist())\n",
    "\n",
    "# Also check if y_validation is actually a DataFrame or numpy array:\n",
    "print(\"y_validation type:\", type(y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fc0f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### PLOTTING FUNCTION ###############\n",
    "\n",
    "def figure_3_observed_vs_predicted_external(y_true, predictions_dict, \n",
    "                                  r2_method='variance_weighted',\n",
    "                                  output_path=f\"{DATA_ROOT}/Saved figures/\"):\n",
    "    \"\"\"\n",
    "    Generate observed vs. predicted scatterplot with Pearson correlation, R2, RMSE and MAE.\n",
    "    \"\"\"\n",
    "    set_publication_style()\n",
    "    fig, axes = plt.subplots(1, 3, figsize=FIGSIZE_TRIPLE)\n",
    "    model_names = list(predictions_dict.keys())\n",
    "    \n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        ax = axes[idx]\n",
    "        y_pred = predictions_dict[model_name]\n",
    "        \n",
    "        # Flatten arrays for scatter plot\n",
    "        y_true_flat = np.asarray(y_true).ravel()\n",
    "        y_pred_flat = np.asarray(y_pred).ravel()\n",
    "        \n",
    "        # Compute metrics based on specified method\n",
    "        if r2_method == 'variance_weighted' or r2_method == 'flattened':\n",
    "            r2 = r2_score(y_true_flat, y_pred_flat)\n",
    "            r2_label = \"R² (var-w)\" # Shortened for display fit\n",
    "        elif r2_method == 'uniform_average':\n",
    "            r2 = r2_score(y_true, y_pred, multioutput='uniform_average')\n",
    "            r2_label = \"R² (uni-avg)\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown r2_method: {r2_method}\")\n",
    "        \n",
    "        # Pearson correlation\n",
    "        pearson_r, p_value = pearsonr(y_true_flat, y_pred_flat)\n",
    "\n",
    "        # --- NEW: Calculate RMSE and MAE for the plot ---\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_flat, y_pred_flat))\n",
    "        mae = mean_absolute_error(y_true_flat, y_pred_flat)\n",
    "        \n",
    "        # Scatter plot\n",
    "        ax.scatter(y_true_flat, y_pred_flat, alpha=0.5, s=5, \n",
    "                   color=MODEL_COLORS.get(model_name, '#1f77b4'),\n",
    "                   edgecolors='none')\n",
    "        \n",
    "        # Perfect prediction diagonal line\n",
    "        min_val = min(y_true_flat.min(), y_pred_flat.min())\n",
    "        max_val = max(y_true_flat.max(), y_pred_flat.max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'k--', \n",
    "                lw=1, alpha=0.5, label='Perfect prediction')\n",
    "        \n",
    "        # Fit regression line\n",
    "        z = np.polyfit(y_true_flat, y_pred_flat, 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_line = np.linspace(y_true_flat.min(), y_true_flat.max(), 100)\n",
    "        y_line = p(x_line)\n",
    "        ax.plot(x_line, y_line, color=\"#000000\",\n",
    "                lw=1.5, alpha=0.8, label='Linear fit')\n",
    "        \n",
    "        # Labels and formatting\n",
    "        ax.set_xlabel('Observed Expression (Log10)', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Predicted Expression (Log10)', fontsize=12, fontweight='bold')\n",
    "        ax.set_title(model_name, fontsize=13, fontweight='bold')\n",
    "        \n",
    "        # --- MODIFIED: Add RMSE and MAE to text box ---\n",
    "        # Construct the string part by part for clarity\n",
    "        stats_text = (f\"Pearson's R = {pearson_r:.4f}\\n\"\n",
    "                      f\"{r2_label} = {r2:.4f}\\n\"\n",
    "                      f\"RMSE = {rmse:.4f}\\n\"\n",
    "                      f\"MAE = {mae:.4f}\")\n",
    "        \n",
    "        # Append p-value logic\n",
    "        if p_value < 0.001:\n",
    "            textstr = f\"{stats_text}\\np < 0.001\"\n",
    "        else:\n",
    "            textstr = f\"{stats_text}\\np = {p_value:.3f}\"\n",
    "        \n",
    "        ax.text(0.05, 0.95, textstr, transform=ax.transAxes, \n",
    "                fontsize=10, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "        \n",
    "        ax.legend(loc='lower right', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        ax.set_xlim(0, 6)\n",
    "        ax.set_ylim(0, 6)\n",
    "    \n",
    "    # counting how many points are within visible range\n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        ax = axes[idx]\n",
    "        xlim = ax.get_xlim()\n",
    "        ylim = ax.get_ylim()\n",
    "    \n",
    "        y_pred = predictions_dict[model_name]\n",
    "        y_true_flat = np.asarray(y_true).ravel()\n",
    "        y_pred_flat = np.asarray(y_pred).ravel()\n",
    "    \n",
    "        outside_x = ((y_true_flat < xlim[0]) | (y_true_flat > xlim[1])).sum()\n",
    "        outside_y = ((y_pred_flat < ylim[0]) | (y_pred_flat > ylim[1])).sum()\n",
    "        \n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  Axis limits: x={xlim}, y={ylim}\")\n",
    "        print(f\"  Points outside x-range: {outside_x}\")\n",
    "        print(f\"  Points outside y-range: {outside_y}\")\n",
    "        print(f\"  Data range: x=[{y_true_flat.min():.2f}, {y_true_flat.max():.2f}], y=[{y_pred_flat.min():.2f}, {y_pred_flat.max():.2f}]\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=DPI, bbox_inches='tight')\n",
    "    print(f\"Figure 1 saved to {output_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Return metrics for reference\n",
    "    metrics_summary = {}\n",
    "    for model_name in model_names:\n",
    "        y_pred = predictions_dict[model_name]\n",
    "        y_true_flat = np.asarray(y_true).ravel()\n",
    "        y_pred_flat = np.asarray(y_pred).ravel()\n",
    "        \n",
    "        pearson_r, p_value = pearsonr(y_true_flat, y_pred_flat)\n",
    "        r2_variance_weighted = r2_score(y_true_flat, y_pred_flat)\n",
    "        r2_uniform = r2_score(y_true, y_pred, multioutput='uniform_average')\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_flat, y_pred_flat))\n",
    "        mae = mean_absolute_error(y_true_flat, y_pred_flat)\n",
    "        \n",
    "        metrics_summary[model_name] = {\n",
    "            'pearson_r': pearson_r,\n",
    "            'p_value': p_value,\n",
    "            'r2_variance_weighted': r2_variance_weighted,\n",
    "            'r2_uniform_average': r2_uniform,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae\n",
    "        }\n",
    "    \n",
    "    return metrics_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e32c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### UNIT TEST ###############\n",
    "\n",
    "# How many MLR predictions are in the \"visible\" range of your plot (roughly 0-7)?\n",
    "visible_range = (mlr_y_pred_val >= -1) & (mlr_y_pred_val <= 7)\n",
    "print(f\"MLR predictions in visible range: {visible_range.sum()} / {mlr_y_pred_val.size}\")\n",
    "print(f\"Percentage visible: {100 * visible_range.sum() / mlr_y_pred_val.size:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00afbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### UNIT TEST ###############\n",
    "\n",
    "print(\"Prediction variance:\")\n",
    "for model_name, y_pred in predictions_validation.items():\n",
    "    print(f\"{model_name}: {y_pred.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b427c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### PLOTTING STEP ###############\n",
    "\n",
    "figure_3_observed_vs_predicted_external(y_validation,predictions_validation,r2_method='variance_weighted')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
